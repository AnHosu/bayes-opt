---
title: ""
bibliography: references.bib
csl: ../citation_style.csl
format:
  html:
    fig-width: 8
    fig-height: 5
    theme: default
html-math-method:
  method: katex
---

[Bayesian optimisation](../bayes-opt-r) is a powerful optimisation technique for black-box functions and processes with expensive evaluations. It is popular for hyperparameter tuning and model selection in machine learning, but has many real-world applications as well. One of the key components of Bayesian optimisation is the initial experiment design, which forms the foundation for the first fit of the surrogate model. In this blog post, we will discuss the importance of initial experiment designs in Bayesian optimisation and dive into a few examples of initial designs.

Along with the discussion are implementations of the designs in R, using only base R and the Tidyverse.

```{r func, echo=TRUE}
library(ggplot2)
library(magrittr)
set.seed(4444)
```

## Initial Designs in Bayesian Optimisation

Bayesian optimization works by constructing a surrogate probabilistic model, typically a Gaussian process, of the objective function. The surrogate model is then used to guide the search for an optimum. The initial experiment design influences this process in two major ways: it provides initial data points for fitting the surrogate model and it is the initial trade off between exploration and exploitation.

In the initial design, we decide on a set of $n$ observations $\mathbf{X} = \{\mathbf{x}_1, \dots, \mathbf{x}_n\}$. Each point, $\mathbf{x}$, is somewhere in the search space, $\mathcal{X} \in \mathbb{R}^d$, with dimensionality $d$. Ideally, we want to restrict the optimisation to a subset of $\mathbb{R}^d$ and the initial design can help achieve that by keeping the observations within a realistic range. For the designs mentioned here, the search space $\mathcal{X} = [0,1]^d$ is assumed. It is a good idea to normalise the input dimensions, when possible. When normalised inputs are not an option, the range $[0,1]$ can easily be scaled to a broader range.

Considering that the experimental budget is limited and that it is expensive to evaluate the objective function, the number of initial training observations should be limited. However, considering that the surrogate models generally are not great for extrapolation, the number of initial observation should not be too small either.

In general, the initial training data should be chosen to provide a good representation of the objective function. This means that the data should be chosen to cover the range of each input dimension. The data should also include inputs that are expected to be both good and bad performers.

An initial design might not always be necessary. If the Gaussian process was defined with a very strong prior, then it could be applied to generate experiments right away. For some processes, historical data might be available to get the experimentation started. Previous experience, experiments, or datasets might also be useful for initialising Bayesian optimisation [@Feurer2015].

Most often though, we do not really know where to start. At best, we might have a vague idea of the general effect of our input features on the measured output. In these cases, we can apply a design, so let's look at some examples.

```{r}
# Generate a 2D designs with 10 samples
n <- 10
d <- 2
```

```{r}
plot_design <- function(samples, title = "") {
  data.frame(samples) %>%
  ggplot() +
  geom_point(aes(x = X1, y = X2), size = 3) +
  xlim(0, 1) +
  ylim(0, 1) +
  theme_minimal() +
  labs(
    title = title,
    x = "Dimension 1",
    y = "Dimension 2"
  )
}
```



## Random Sampling

As the name suggests this design is completely random. Each point is simply sampled from the uniform distribution [@McKay1979]

$$\mathbf{x} \sim \text{Unif}(0, 1)$$

```{r}
random_sampling <- function(n, d) matrix(runif(n * d), n, d)
```

```{r}
samples <- random_sampling(n, d)
# Create a scatter plot of the samples
plot_design(samples, "Random Sampling")
```

Random sampling represents a focus on exploration in the face of uncertainty. The main advantage of the design is its ease of implementation. The main drawback is that it is not guaranteed to efficiently sample the search space - there is a chance of points with close proximity, which is a waste we usually cannot afford in Bayesian optimisation. A Latin Hypercube Design can remedy this drawback.

## Latin Hypercube Sampling (LHS)

Latin Hypercube Sampling (LHS) is a popular stratified sampling technique that ensures a balanced distribution of samples across the search space. The key idea behind LHS is to divide each dimension of the search space into equally sized intervals and randomly sample one point from each interval. LHS still samples each point from a uniform distribution, but restricts the intervals for each point [@McKay1979].

Given a search space defined by $n$ intervals in each of the $d$ dimensions, LHS generates $n$ samples, where the $i^{th}$ sample $x_i$ is defined as:

$$x_i = (x_{i1}, x_{i2}, \cdots, x_{id})$$

Each coordinate $x_{ij}$ is chosen by sampling from the corresponding $j^{th}$ interval. This ensures that each interval is used exactly once for each dimension, resulting in a well-distributed set of samples.

```{r}
#' Latin Hypercube Sampling
#'
#' @param n <int> number of observations
#' @param d <int> number of features
#'
#' @return matrix of shape (n,d)
latin_hypercube_sampling <- function(n, d) {
  samples <- matrix(nrow = n, ncol = d)
  for (i in 1:d) samples[, i] <- sample(n) + runif(n)
  (samples - 1) / n
}

```

Advantages

    LHS ensures a uniform coverage of the search space by construction, which promotes exploration.
    It is relatively simple to implement and computationally inexpensive.

One of the main advantages of LHS is that it ensures a uniform coverage of the search space by construction. This promotes exploration in the optimization process, reducing the risk of missing potential optima. Furthermore, LHS is relatively simple to implement and computationally inexpensive, making it an attractive choice for many optimization problems.

Limitations

    LHS is a random sampling technique and does not take into account any prior knowledge about the objective function.
    In high-dimensional problems, the uniform coverage property of LHS may not be sufficient for a good initial design.

Despite its benefits, LHS does have some limitations. As a random sampling technique, it does not take into account any prior knowledge about the objective function, which may result in inefficient sampling for some problems. Additionally, in high-dimensional problems, the uniform coverage property of LHS may not be sufficient for a good initial design, and other techniques like Sobol sequences or more advanced space-filling designs might be more appropriate.

#### LHS in 2D

```{r}

samples <- latin_hypercube_sampling(n, d)
# Create a scatter plot of the samples
data.frame(samples) %>%
  ggplot() +
  geom_point(aes(x = X1, y = X2), size = 3) +
  xlim(0, 1) +
  ylim(0, 1) +
  theme_minimal() +
  labs(
    title = "2D Latin Hypercube Sampling",
    x = "Dimension 1",
    y = "Dimension 2"
  )
```

## Maxmin Design

Maxmin design, also known as the maximum-minimum distance design, is an initial experiment design technique that aims to maximize the minimum distance between any two points in the initial set of experiments. By ensuring that the experiments are well-separated, maxmin design promotes exploration throughout the search space.

Mathematically, given a set of $n$ samples, $X = {x_1, x_2, \cdots, x_n}$, in a $d$-dimensional search space, the maxmin design seeks to find the set of samples that maximizes the minimum pairwise distance:

$$\max_X \min_{i \neq j} \|x_i - x_j\|$$

```{r}
maxmin_design <- function(n, d) {
  samples <- matrix(nrow = n, ncol = d)
  samples[1,] <- runif(d) # Initialize with a random point
  
  for (i in 2:n) {
    candidate_points <- matrix(runif(1000 * d), ncol = d)
    min_dists <- apply(candidate_points, 1, function(p) {
      min(sqrt(rowSums((t(samples[1:(i-1),]) - p)^2)))
    })
    samples[i,] <- candidate_points[which.max(min_dists),]
  }
  
  return(samples)
}
```


Advantages of Maxmin Design

Maxmin design provides several benefits in the context of initial experiment design for Bayesian optimization:

    It promotes exploration by ensuring a well-separated set of samples in the search space, reducing the risk of missing potential optima.
    The design is suitable for problems with unknown or complex underlying distributions, as it does not rely on any assumptions about the distribution of the objective function.

Limitations of Maxmin Design

Despite its advantages, maxmin design has some limitations as well:

    Computing the maxmin design can be computationally expensive, particularly in high-dimensional problems. This is because it involves searching for the set of points that maximizes the minimum pairwise distance, which is a combinatorial optimization problem.
    Like LHS, maxmin design is a random sampling technique and does not take into account any prior knowledge about the objective function, which may result in inefficient sampling for some problems.

Generating a 2D Maxmin Design in R

To create a maxmin design, we can use a greedy algorithm that iteratively adds points to the design while maximizing the minimum distance to the existing points. Here's a simple R implementation:

```{r}
samples <- maxmin_design(n, d)
# Create a scatter plot of the samples
ggplot() +
  geom_point(data = data.frame(samples), aes(x = X1, y = X2), size = 3) +
  theme_minimal() +
  labs(title = "2D Maxmin Design",
       x = "Dimension 1", y = "Dimension 2")
```

## Deterministic Design

This design is based entirely on established practices or assumptions about a the objective function or process.

## Augmented Latin Hypercube Sampling

This design expands on LHS to add prior knowledge or assumptions to the initial design.

## Designing with Discrete or Categorical Features

Assign discrete feature to intervals in range [0,1].

One-hot encoding of categorical features. Use continuous design with softmax on one-hot features or maximum gets 1.


# References {-}

<div id="refs"></div>

# License

The content of this project itself is licensed under the [Creative Commons Attribution-ShareAlike 4.0 International license](https://creativecommons.org/licenses/by-sa/4.0/), and the underlying code is licensed under the [GNU General Public License v3.0 license](https://github.com/AnHosu/bespoke-bayesian-biochem/blob/main/LICENSE).
