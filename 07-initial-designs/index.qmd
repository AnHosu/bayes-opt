---
title: ""
bibliography: references.bib
csl: ../citation_style.csl
format:
  html:
    fig-width: 8
    fig-height: 5
    theme: default
html-math-method:
  method: katex
---

[Bayesian optimisation](../bayes-opt-r) is a powerful optimisation technique for black-box functions and processes with expensive evaluations. It is popular for hyperparameter tuning in machine learning, but has many real-world applications as well. One of the key components of Bayesian optimisation is the initial experiment design, which forms the foundation for the first fit of the surrogate model. In this post, we will discuss the importance of initial experiment designs in Bayesian optimisation and dive into a few different ways to sample such initial designs.

Along with the discussion are example implementations in R.

```{r func, echo=TRUE}
library(ggplot2)
library(magrittr)
set.seed(4444)
```

## Initial Experiment Designs in Bayesian Optimisation

Bayesian optimisation works by constructing a surrogate probabilistic model, typically a Gaussian process, of the objective function. The surrogate model is then used to guide the search for an optimum through sequential experimentation. The initial experiment design influences this process in two major ways: it provides initial data points for fitting the surrogate model and it is the initial trade off between exploration and exploitation.

In the initial design, we decide on a set of $n$ observations $\mathbf{X} = \{\mathbf{x}_1, \dots, \mathbf{x}_n\}$. Each point, $\mathbf{x}$, is somewhere in the search space, $\mathcal{X} \in \mathbb{R}^d$, with dimensionality $d$. Ideally, we want to restrict the optimisation to a subset of $\mathbb{R}^d$ and the initial design can help achieve that by keeping the observations within a realistic range. For the designs mentioned here, the search space $\mathcal{X} = [0,1]^d$ is assumed. It is a good idea to normalise the input dimensions, but when normalised inputs are not an option, the range $[0,1]$ can easily be scaled to a broader range.

Considering that the experimental budget is limited and that it is expensive to evaluate the objective function, the number of initial training observations should be limited. However, considering that the surrogate models generally are not great for extrapolation, the number of initial observations should not be too small either.

In general, the initial training data should be chosen to provide a good representation of the objective function. This means that the data should be chosen to cover the range of each input dimension. The data should also include inputs that are expected to be both good and bad performers.

An initial design might not always be necessary. If the Gaussian process was defined with a very strong prior, then it could be applied to generate experiments right away. For some processes, historical data might be available to get the experimentation started. Previous experience, experiments, or datasets might also be useful for initialising Bayesian optimisation [@Feurer2015].

Most often though, we do not really know where to start. At best, we might have a vague idea of the general effect of our input features on the measured output. In these cases, we can apply a design, so let's look at some examples.

#### An Example Case

To demonstrate each design, we imagine a situation where we are doing Bayesian optimisation on an objective process with two input dimensions. We have decided that we want to take 10 initial samples.

```{r}
# 2D design with 10 samples
n <- 10
d <- 2
```

We also define a simple plot to visualise the design.

```{r}
plot_design <- function(samples, title = "") {
  data.frame(samples) %>%
  ggplot() +
  geom_point(aes(x = X1, y = X2), size = 3) +
  theme_minimal() +
  labs(
    title = title,
    x = "Dimension 1",
    y = "Dimension 2"
  )
}
```

Now we are ready to look at our first design.

## Random Sampling

The first, and arguably most simple, example of an initial design is Random Sampling. As the name suggests this design is completely random. Each dimension for each point is simply sampled from the uniform distribution [@McKay1979]

$$\mathbf{x} \sim \text{Unif}(0, 1)$$
The implementation is just a single line:

```{r}
random_sampling <- function(n, d) matrix(runif(n * d), n, d)
```

Random sampling represents a focus on exploration in the face of uncertainty. The main advantage of the random design is its ease of implementation, but it is also important to note that it is trivial to add additional samples to this design. The main drawback is that it is not guaranteed to efficiently sample the search space - there is a chance of points with close proximity, which is a waste we usually cannot afford in Bayesian optimisation. A Latin Hypercube Design can remedy this drawback.

#### Random Sampling in 2D

Here is a random sampling design for our imagined case.

```{r random_sampling, fig.width=6, fig.height=6}
samples <- random_sampling(n, d)
plot_design(samples, "Random Sampling")
```

The main thing to notice is that we there are clusters of points and large swaths of empty space. This seems a bit wasteful, when each evaluation of the objective function is expensive.

## Latin Hypercube Sampling

Latin Hypercube Sampling (LHS) is a stratified sampling technique that ensures a balanced distribution of samples across the search space. The key idea behind LHS is to divide each dimension of the search space into equally sized intervals and randomly sample one point from each interval. LHS still samples each point from a uniform distribution, but restricts the intervals for each point [@McKay1979].

Given a search space with $d$ dimensions or features, we want to sample $n$ points, such that they satisfy a Latin Hypercube of evenly spaced intervals.

We do this by dividing each dimension into $n$ equally spaced intervals. Then, for each dimension, we sample a random permutation of the numbers $1, ..., n$ and the resulting sequence determines which interval of that dimension is sampled for each of the $n$ samples. This ensures that each interval in each dimension is sampled exactly once. Finally a sample is drawn from each of the chosen intervals.

Let $x_{j,i}$, $j = 1, ..., n$, $i = 1, ..., d$ be the $i^{\text{th}}$ dimension of the $j^{\text{th}}$ observation. Let $\mathbf{\pi}_i$, $i = 1, ..., d$ be $d$ independent random permutations of the numbers $\{1,...,n\}$, then

$$\mathbf{x}_{i,:} = \frac{\mathbf{\pi}_i - 1 + \mathbf{\nu}_i}{n}$$

Where $\mathbf{x}_{i,:}$ are the $i^{\text{th}}$ dimension of all observations, and

$$\mathbf{\nu}_i \sim \text{Unif}(0, 1)$$

Here is an implementation of LHS:

```{r}
#' Latin Hypercube Sampling
#'
#' @param n <int> number of observations
#' @param d <int> number of features
#'
#' @return matrix of shape (n,d)
latin_hypercube_sampling <- function(n, d) {
  samples <- matrix(nrow = n, ncol = d)
  for (i in 1:d) samples[, i] <- sample(n) + runif(n)
  (samples - 1) / n
}
```


LHS ensures a uniform coverage of the search space, which is great for efficient exploration. Furthermore, it is relatively simple to implement, so it is an easy place to start.

A large drawback of LHS is that additional points cannot be sampled after the design is complete. Imagine the very likely scenario that we are about to do Bayesian optimisation we chose initial number of observations, $n$, that was too small, so we are unable to fit a useful surrogate model. We have already done the $n$ observations and they are good, but we would like to add just a few more, $m$, initial observations before starting optimisation in earnest. Unfortunately, we cannot add the $m$ points to the original LHS. Neither is the LHS of size $n$ a subset of the LHS of size $n + m$. To get a LHS of size $n + m$, we would have to create an entirely new design. The intuition for this is that, as part of the design, we split each dimension into $n$ equally spaced intervals, so we cannot retroactively split them into $n + m$, once the $n$ samples are done. Another way to think about it is that the design is generated in a for loop over each dimension, so the total number of samples must be known from the beginning. In practice, we could use some space-filling criterion or just random sampling to generate the $m$ points, but they would not fit in the Latin Hypercube.

#### LHS in 2D

```{r lhs, fig.width=6, fig.height=6}
samples <- latin_hypercube_sampling(n, d)
plot_design(samples, "Latin Hypercube Sampling")
```

The main thing to notice in this design is the neat stratification of each dimension.

## Quasi-Random Design


#### Sobol Sequence in 2D

```{r sobol, fig.width=6, fig.height=6}
samples <- randtoolbox::sobol(n, d)
plot_design(samples, "Quasi-Random Design")
```


## Augmented Latin Hypercube Sampling

This design expands on LHS to add prior knowledge or assumptions to the initial design.

```{r scale, fig.width=6, fig.height=6}
l <- latin_hypercube_sampling(1e3, 2)
l[,1] <- qnorm(l[,1])
l[,2] <- qexp(l[,2])
plot_design(l, "yay")
```


## Designing with Discrete or Categorical Features

Assign discrete feature to intervals in range [0,1].

One-hot encoding of categorical features. Use continuous design with softmax on one-hot features or maximum gets 1.


# References {-}

<div id="refs"></div>

# License

The content of this project itself is licensed under the [Creative Commons Attribution-ShareAlike 4.0 International license](https://creativecommons.org/licenses/by-sa/4.0/), and the underlying code is licensed under the [GNU General Public License v3.0 license](https://github.com/AnHosu/bespoke-bayesian-biochem/blob/main/LICENSE).
