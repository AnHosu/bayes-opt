---
title: ""
bibliography: references.bib
csl: ../citation_style.csl
format:
  html:
    fig-width: 8
    fig-height: 5
    theme: default
html-math-method:
  method: katex
---

Bayesian optimisation is a powerful optimisation technique for black-box functions and processes with expensive evaluations. It is popular for hyperparameter tuning and model selection in machine learning, but has many real-world applications as well. One of the key components of Bayesian optimisation is the acquisition function, which guides the search process by balancing exploration and exploitation of the search space. In this post, we will dive into the role of acquisition functions in Bayesian optimisation and discuss some popular examples of acquisition functions.

```{r func, echo=TRUE}
library(ggplot2)
library(magrittr)
set.seed(4444)
```

## Acquisition Functions in Bayesian Optimisation

Bayesian optimisation is an iterative process. It combines a probabilistic surrogate model, often a Gaussian Process (GP), with an acquisition function to select the next point to evaluate in an expensive objective function or process, $f$. The surrogate model captures our current understanding and uncertainty of the objective function, while the acquisition function helps balance the trade-off between exploring new regions of input space and exploiting regions with high predicted performance.

Mathematically, the acquisition function, $a(\mathbf{x})$, assigns a value to each point in the search space $\mathbf{x} \in \mathcal{X}$. The next point to evaluate, $\mathbf{x}_{t+1}$, is chosen by maximising or minimising the acquisition function, depending on the optimisation task at hand:

$$\mathbf{x}_{t+1} = \arg\min_{\mathbf{x} \in \mathcal{X}} a(\mathbf{x})$$

The acquisition function takes into account both the mean $\mu(\mathbf{x})$ and the variance $\sigma^2(\mathbf{x})$ of the surrogate model's prediction, to balance exploration and exploitation. Roughly speaking, areas with extreme values of $\mu(\mathbf{x})$ correspond to areas we might exploit to get good performing samples, while areas with high values of $\sigma^2(\mathbf{x})$ correspond to with high uncertainty that we might consider for exploration.

#### Notation

The notation used in this post is as follows 

$a(\mathbf{x})$ is an acquisition function of a point $\mathbf{x}$ in the search space $\mathcal{X}$

$f(\mathbf{x})$ is the value of true objective function, $f$, at $\mathbf{x}$. It is this function that we aim to optimise. However, the function is not directly available and it is expensive to evaluate so we use a surrogate model to approximate it.

In most applications, the observations of the objective function are noisy, $y = f(\mathbf{x}) + \epsilon$, where $\epsilon$ is Gaussian noise. So we will use $\mathbf{y}$ to indicate observations.

$f(\mathbf{x}^+)$, $f_{\min}$, and $f_{\max}$ all represent the best observed value of the objective function so far. If the observations are noisy, the corresponding notation is $y^+$, $y_{\min}$, and $y_{\max}$. For most examples of acquisition functions, this post focuses on minimisation problems, where the best observed value is $y_{\min}$.

$\mu(\mathbf{x})$ represents the mean prediction of the surrogate model at point $\mathbf{x}$.

$\sigma(\mathbf{x})$ represents the standard deviation (uncertainty) of the surrogate model's prediction at point $\mathbf{x}$. For a GP, this is an entry in the diagonal of the posterior covariance matrix.

## Examples of Acquisition Functions

Anatomy of an acquisition function

How to estimate them

The examples are given with minimisation in mind

Example problem

For the demonstration of acquisition functions, we are going to need a toy problem. We will use a simple objective function without noise.

```{r}
objective_function <- function(x) {
  sin(12 * x) * x + 0.5 * x^2
}
X_pred <- matrix(seq(0, 1, length.out = 100), 100, 1)
y_pred <- objective_function(X_pred)
```

This function has two minima, so it will not be too easy to minimise.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
ggplot(data = tibble::tibble(x = X_pred, y = y_pred)) +
  geom_line(aes(x = x, y = y)) +
  theme_minimal() +
  labs(x = "x", y = "f(x)", title = "Objective Function")
```

We will approximate the the objective function with a Gaussian process surrogate model that utilises the RBF kernel. See the [Bayesian optimisation post](../bayes-opt-r) for details.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#' RBF Kernel
#'
#' @param X1 matrix of dimensions (n, d). Vectors are coerced to (1, d).
#' @param X2 matrix of dimensions (m, d). Vectors are coerced to (1, d).
#' @param l length scale
#' @param sigma_f scale parameter 
#'
#' @return matrix of dimensions (n, m)
rbf_kernel <- function(X1, X2, l = 1.0, sigma_f = 1.0) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  sqdist <- (- 2*(X1 %*% t(X2))) %>%
    add(rowSums(X1**2, dims = 1)) %>%
    sweep(2, rowSums(X2**2, dims = 1), `+`)
  sigma_f**2 * exp(-0.5 / l**2 * sqdist)
}

#' Random Samples from a Multivariate Gaussian
#' 
#' This implementation is similar to MASS::mvrnorm, but uses chlosky
#' decomposition instead. This should be more stable but is less efficient than
#' the MASS implementation, which recycles the eigen decomposition for the
#' sampling part.
#'
#' @param n number of samples to sample
#' @param mu the mean of each input dimension
#' @param sigma the covariance matrix
#' @param epsilon numerical tolerance added to the diagonal of the covariance
#'  matrix. This is necessary for the Cholesky decomposition, in some cases.
#'
#' @return numerical vector of n samples
rmvnorm <- function(n = 1, mu, sigma, epsilon = 1e-6) {
    p <- length(mu)
    if(!all(dim(sigma) == c(p, p))) stop("incompatible dimensions of arguments")
    ev <- eigen(sigma, symmetric = TRUE)$values
    if(!all(ev >= -epsilon*abs(ev[1L]))) {
      stop("The covariance matrix (sigma) is not positive definite")
    }
    cholesky <- chol(sigma + diag(p)*epsilon)
    sample <- rnorm(p*n, 0, 1)
    dim(sample) <- c(n, p)
    sweep(sample %*% cholesky, 2, mu, FUN = `+`)
}

#' Get Parameters of the Posterior Gaussian Process
#'
#' @param kernel kernel function used for the Gaussian process
#' @param X_pred matrix (m, d) of prediction points
#' @param X_train matrix (n, d) of training points
#' @param y_train column vector (n, d) of training observations
#' @param noise scalar of observation noise
#' @param ... named parameters for the kernel function
#'
#' @return list of mean (mu) and covariance (sigma) for the Gaussian
posterior <- function(kernel, X_pred, X_train, y_train, noise = 1e-8, ...) {
  if (is.null(dim(X_pred))) dim(X_pred) <- c(length(X_pred), 1)
  if (is.null(dim(X_train))) dim(X_train) <- c(length(X_train), 1)
  if (is.null(dim(y_train))) dim(y_train) <- c(length(y_train), 1)
  K <- kernel(X_train, X_train, ...) + noise**2 * diag(dim(X_train)[[1]])
  K_s <- kernel(X_train, X_pred, ...)
  K_ss <- kernel(X_pred, X_pred, ...) + 1e-8 * diag(dim(X_pred)[[1]])
  K_inv <- solve(K)
  mu <- (t(K_s) %*% K_inv) %*% y_train
  sigma <- K_ss - (t(K_s) %*% K_inv) %*% K_s
  list(mu = mu, sigma = sigma)
}

#' Negative log-Likelihood of a Kernel
#'
#' @param kernel kernel function
#' @param X_train matrix (n, d) of training points
#' @param y_train column vector (n, d) of training observations
#' @param noise scalar of observation noise
#'
#' @return function with kernel parameters as input and negative log likelihood
#' as output
nll <- function(kernel, X_train, y_train, noise) {
  function(params) {
    n <- dim(X_train)[[1]]
    K <- rlang::exec(kernel, X1 = X_train, X2 = X_train, !!!params)
    L <- chol(K + noise**2 * diag(n))
    a <- backsolve(r = L, x = forwardsolve(l = t(L), x = y_train))
    0.5*t(y_train)%*%a + sum(log(diag(L))) + 0.5*n*log(2*pi)
  }
}

#' Gaussian Process Regression
#'
#' @param kernel kernel function
#' @param X_train matrix (n, d) of training points
#' @param y_train column vector (n, d) of training observations
#' @param noise scalar of observation noise
#' @param ... parameters of the kernel function with initial guesses. Due to the
#' optimiser used, all parameters must be given and the order unfortunately
#' matters
#'
#' @return function that takes a matrix of prediction points as input and
#' returns the posterior predictive distribution for the output
gpr <- function(kernel, X_train, y_train, noise = 1e-8, ...) {
  kernel_nll <- nll(kernel, X_train, y_train, noise)
  param <- list(...)
  opt <- optim(par = rep(1, length(param)), fn = kernel_nll)
  opt_param <- opt$par
  function(X_pred) {
    post <- rlang::exec(
      posterior,
      kernel = kernel,
      X_pred = X_pred,
      X_train = X_train,
      y_train = y_train,
      noise = noise,
      !!!opt_param
    )
    list(
      mu = post$mu,
      sigma = diag(post$sigma),
      parameters = set_names(opt_param, names(param))
    )
  }
}
```

The model will receive four training points. We perform Gaussian process regression and condition the Gaussian process on our training data before drawing from the posterior predictive distribution on a grid of $\mathbf{x} \in \mathcal{X}$. The $\mu$ and $\sigma$ of the posterior predictive distribution are needed to calculate some acquisition functions.

```{r}
X_train <- matrix(c(0.1, 0.2, 0.7, 0.75), 4, 1)
y_train <- objective_function(X_train)
gp <- gpr(rbf_kernel, X_train, y_train, noise = 1e-8, l = 1, sigma_f = 1)
y_best <- min(y_train)
post_pred <- gp(X_pred)
mu <- post_pred$mu
sigma <- post_pred$sigma
```

Here is what the Gaussian process looks like so far.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
gp_plot <- tibble::tibble(
    mu = mu,
    uncertainty = 1.96*sqrt(sigma),
    upper = mu + uncertainty,
    lower = mu - uncertainty,
    x = X_pred,
    f = y_pred
  ) %>%
    ggplot(aes(x = x)) +
    geom_line(aes(y = mu, colour = "Mean")) +
    geom_ribbon(
      aes(ymin = lower, ymax = upper, fill = "89% interval"),
      alpha = 0.2
    ) +
    geom_point(
      data = tibble::tibble(x = X_train, y = y_train),
      aes(x = x, y = y, shape = "Training point"),
      colour = "#fb8500",
      size = 4
    ) +
    geom_line(mapping = aes(y = f, colour = "True function")) +
    scale_shape_manual(values = c("Training point" = "+")) +
    scale_fill_manual(values = c("89% interval" = "#219ebc")) +
    labs(shape = "") +
    theme_minimal() +
    labs(
      y = "y",
      x = "",
      colour = "",
      fill = ""
    ) +
    theme(panel.grid = element_blank(), axis.text.x = element_blank())
gp_plot
```

Now we are ready to apply different acquisition functions to help recommend the next point to sample. We also define a plot to visualise the acquisition function along with the GP.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
acquisition_plot <- function(X_pred, ei, gp_plot, label = "EI", title = "") {
  p1 <- tibble::tibble(
    x = X_pred,
    ei = ei
  ) %>%
    ggplot() +
    geom_line(aes(x = x, y = ei, colour = label)) +
    theme_minimal() +
    labs(x = "", y = label, colour = "") +
    theme(panel.grid = element_blank())
  p2 <- gp_plot + labs(title = title)
  aligned_plots <- cowplot::align_plots(p2, p1 , align = "v")
  cowplot::plot_grid(aligned_plots[[1]], aligned_plots[[2]], ncol = 1)
}
```

#### Expected Improvement

The basic idea behind Expected Improvement (EI) is to search for the point in the search space that has the highest probability of improving the current best solution. EI is defined as the expected value of the improvement over the current best solution, where the improvement is defined as the difference between the function value at the candidate point and the current best value. In other words, EI measures how much better the objective function is expected to be at the candidate point compared to the current best value, weighted by the probability of achieving that improvement.

Formally, the expected improvement acquisition function for a minimisation problem is defined as:

$$a_{EI}(\mathbf{x}) = \mathbb{E}\left[\max(0, f_{\min} - f(\mathbf{x}))\right]$$

where $\mathbf{x}$ is the candidate point and $f_{\min}$ is the current best function value observed so far. 

When using a GP surrogate model conditioned on noisy observations in place of $f$, EI can be calculated using the formula 

$$a_{EI}(\mathbf{x}) = (y_{min} - \mu(\mathbf{x}) - \xi) \Phi(Z) + \sigma(\mathbf{x}) \phi(Z)$$

with

$$Z = \frac{y_{min} - \mu(\mathbf{x}) - \xi}{\sigma(\mathbf{x})}$$

where $\mu(\mathbf{x})$ and $\sigma(\mathbf{x})$ are the mean and standard deviation of the Gaussian process at $\mathbf{x}$. $\Phi$ and $\phi$ are the standard normal cumulative distribution function and probability density function, respectively, and $\xi$ is a trade-off parameter that balances exploration and exploitation. Higher values of $\xi$ lead to more exploration and smaller values to exploitation. $a_{EI}(\mathbf{x}) = 0$ when $\sigma(\mathbf{x}) = 0$.

The formulas can be implemented directly.

```{r}
#' Expected Improvement Acquisition Function
#' 
#' @mu vector of length m. Mean of a Gaussian process at m points.
#' @sigma vector of length m. The diagonal of the covariance matrix of a
#' Gaussian process evaluated at m points.
#' @param y_best scalar. Best mean prediction so far on observed points
#' @param xi scalar, exploration/exploitation trade off
#' @task one of "max" or "min", indicating the optimisation problem
#'
#' @return EI, vector of length m
expected_improvement <- function(mu, sigma, y_best, xi = 0.01, task = "min") {
  if (task == "min") imp <- y_best - mu - xi
  if (task == "max") imp <- mu - y_best - xi
  if (is.null(imp)) stop('task must be "min" or "max"')
  Z <- imp / sigma
  ei <- imp * pnorm(Z) + sigma * dnorm(Z)
  ei[sigma == 0.0] <- 0.0
  ei
}
```

Let's see it in action. We calculate EI along a grid and draw it below the GP.

```{r}
ei <- expected_improvement(mu, sigma, y_best, xi = 0.05)
acquisition_plot(X_pred, ei, gp_plot, "EI", "Expected Improvement")
```

The next sampling point, $x^+$, is the one that maximises the acquisition function, here EI. In this case, this is close to the right edge, where there is a high mean prediction but also high uncertainty, so it satisfies our need for both exploration and exploitation.

#### Probability of Improvement

Probability of improvement (PI) aims to select the point that has the highest probability of improving the current best solution. Like expected improvement, the PI function balances exploration and exploitation by taking into account both the mean and the variance of the surrogate model. A point with a high mean and low variance is likely to be a good candidate for exploitation, while a point with a high variance but lower mean may be more suitable for exploration.

The PI acquisition function is defined as

$$a_{PI}(\mathbf{x}) = P(f(\mathbf{x}) \lt f_{\min} + \xi)$$

When using a GP surrogate model conditioned on noisy observations in place of $f$, EI can be calculated using the formula 

$$a_{\text{PI}}(\mathbf{x}) = \Phi\left(\frac{y_{min} - \mu(\mathbf{x}) - \xi}{\sigma(\mathbf{x})}\right)$$

where $\mu(\mathbf{x})$ and $\sigma(\mathbf{x})$ are the mean and standard deviation of the Gaussian process at $\mathbf{x}$, $\Phi$ is the standard normal cumulative distribution function, and $\xi$ is a trade-off parameter that balances exploration and exploitation. Higher values of $\xi$ lead to more exploration and smaller values to exploitation.

The formula can be implemented directly

```{r}
#' Probability of Improvement Acquisition Function
#' 
#' @mu vector of length m. Mean of a Gaussian process at m points.
#' @sigma vector of length m. The diagonal of the covariance matrix of a
#' Gaussian process evaluated at m points.
#' @param y_best scalar. Best mean prediction so far on observed points
#' @param xi scalar, exploration/exploitation trade off
#' @task one of "max" or "min", indicating the optimisation problem
#'
#' @return PI, vector of length m
probability_of_improvement <- function(mu,
                                       sigma,
                                       y_best,
                                       xi = 0.01,
                                       task = "min") {
  if (task == "min") imp <- y_best - mu - xi
  if (task == "max") imp <- mu - y_best - xi
  pnorm(imp / sigma)
}
```

Let's see it in action. We calculate PI along a grid and draw it below the GP.

```{r}
pi <- probability_of_improvement(mu, sigma, y_best, xi = 0.05)
acquisition_plot(X_pred, pi, gp_plot, "PI", "Probability of Improvement")
```

The next sampling point, $x^+$, is the one that maximises the acquisition function, here PI. In this case, this is close to the right edge, where there is a high mean prediction but also high uncertainty, so it satisfies our need for both exploration and exploitation. There are a few other good contenders in the spaces between training points though.

#### Lower & Upper Confidence Bound

The Lower Confidence Bound (LCB) and Upper Confidence Bound (UCB) acquisition functions are fairly simple acquisition functions. They balance exploration and exploitation by combining the mean and the weighted standard deviation predictions of the surrogate model. LCB is used for minimisation problems and UCP is applied for maximisation problems.

The LCB function is defined as

$$a_{LCB}(\mathbf{x}) = \mu(\mathbf{x}) - \kappa \sigma(\mathbf{x})$$

and UCB as

$$a_{UCB}(\mathbf{x}) = \mu(\mathbf{x}) + \kappa \sigma(\mathbf{x})$$

where $\mu(\mathbf{x})$ and $\sigma(\mathbf{x})$ are the mean and standard deviation of the Gaussian process at $\mathbf{x}$ and $\kappa$ is a tunable parameter that controls the balance between exploration and exploitation. Higher values of $\kappa$ promote more exploration, while lower values emphasise exploitation.

The acquisition functions are straightforward to implement, given $\mu$ and $\sigma$:

```{r}
#' Upper and Lower Confidence Bound Acquisition Function
#' 
#' @mu vector of length m. Mean of a Gaussian process at m points.
#' @sigma vector of length m. The diagonal of the covariance matrix of a
#' Gaussian process evaluated at m points.
#' @param kappa scalar, exploration/exploitation trade off
#' @task one of "max" or "min", indicating the optimisation problem
#'
#' @return CB, vector of length m
confidence_bound <- function(mu, sigma, kappa, task = "min") {
  if (task == "min") return(mu - kappa * sigma)
  if (task == "max") return(mu + kappa * sigma)
}
```

Let's see it in action. We calculate LCB along a grid and draw it below the GP.

#### TODO should LCB be minimised to find x^+? Need good source.

```{r}
cb <- confidence_bound(mu, sigma, kappa = 2, "min")
acquisition_plot(X_pred, cb, gp_plot, "LCB", "Lower Confidence Bound")
```

#### Thompson Sampling

Thompson Sampling is an information-based acquisition function that has gained popularity for its simplicity and effectiveness. It was originally proposed as a method for solving multi-armed bandit problems and was later adapted for Bayesian optimisation. [@garnett_bayesoptbook_2023]

Thompson Sampling works by sampling from the posterior distribution of the surrogate model and selecting the point with the highest (or lowest, for minimization problems) sampled value. This approach encourages exploration in regions with high uncertainty, as the sampled values can vary greatly in these regions. At the same time, it also exploits areas with good performance, as the algorithm will sample points that have a high mean value more frequently.

A Gaussian process represents a distribution over functions. The distribution has been conditioned on training data, $\mathcal{D}$ and We could sample functions from it

$$a_{ts}(\mathbf{x}) \sim p(y | \mathcal{D})$$

The sampled functions could be minimised or maximised, and we define a point, $\mathbf{x}'$, that optimises the function

$$\mathbf{x}' \in \arg\max\limits_{\mathbf{x} \in \mathcal{X}}(a_{ts}(\mathbf{x}'))$$

𝑝 (𝑥∗ | X, y). 

We sample a point from that distribution. This is our next point 

𝑥_t+1 ∼ 𝑝 (𝑥∗ | X, y)

Implementation

Draw a random function from the posterior predictive distribution

Maximise or minimise that function

The point that optimises that function is the new sampling point

```{r}
ts <- as.vector(rmvnorm(1, mu, diag(sigma)))
acquisition_plot(X_pred, ts, gp_plot, "Sample", "Thompson Sampling")
```


#### Mutual Information & Entropy Search

#### Knowledge Gradient

# References {-}

<div id="refs"></div>

# License

The content of this project itself is licensed under the [Creative Commons Attribution-ShareAlike 4.0 International license](https://creativecommons.org/licenses/by-sa/4.0/), and the underlying code is licensed under the [GNU General Public License v3.0 license](https://github.com/AnHosu/bespoke-bayesian-biochem/blob/main/LICENSE).