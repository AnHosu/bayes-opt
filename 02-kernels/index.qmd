---
title: ""
bibliography: references.bib
csl: ../citation_style.csl
format:
  html:
    fig-width: 8
    fig-height: 5
    theme: default
html-math-method:
  method: katex
---

This post takes an extensive look at kernels and discusses the rationales, applications, and limitations of some popular kernels. Along with the discussion are implementations of many of the kernels with R and the Tidyverse.

```{r func, echo=TRUE}
library(ggplot2)
library(magrittr)
set.seed(4444)
```

Kernels, also known as covariance functions or kernel functions, are central to Gaussian processes and Bayesian optimisation. They provide the main means of implementing prior knowledge about the modelled process into Bayesian optimisation or any other machine learning procedure that takes advantage of kernels.

Intuitively, kernels quantify how similar two points are, given just their position in input space. The kernel function determines the smoothness and complexity of the resulting Gaussian process model, and it controls how much weight is given to different regions of the input space. Different types of kernel functions can be used to model different types of data, such as periodic or spatial data.

Bayesian optimisation extensively employs Gaussian processes, so kernels provide the main means of define a prior distribution over the objective function being optimised. There are a plethora of kernels available, and for successful implementations of Bayesian optimisation, selecting the right one is essential but difficult.

## Applying Kernels in Gaussian Processes

Formally, a kernel function $k(\mathbf{x},\mathbf{x'})$ takes two inputs, $\mathbf{x}$ and $\mathbf{x'}$, and returns a real-valued scalar that represents a similarity measure between the inputs.

A kernel function must be positive semi-definite (PSD). This means that the kernel matrix, $\mathbf{\Sigma}$, constructed from any set of $n$ input row vectors ${\mathbf{x}_1, \ldots, \mathbf{x}_n}$, must be PSD. The entries of the kernel matrix are defined as $\mathbf{\Sigma}_{i,j} = k(\mathbf{x}_i, \mathbf{x}_j)$, for all combinations of $i, j \in (1, \ldots, n)$. The PSD property ensures that the kernel matrix can be applied as the covariance matrix in a Gaussian process.

In the context of a Gaussian process that should approximate an objective function, a good kernel function should be flexible enough to capture the underlying structure of the data, but not so flexible that it overfits the data. The choice of kernel function and its hyperparameters can have a significant impact on the performance of the Gaussian process and its application in Bayesian optimisation, so it is important to choose carefully and experiment with different options.

However, without knowing the virtues, pitfalls, and assumptions of a kernel, it is difficult to assess its quality for a given problem. In the following sections, a selection of kernels and their virtues are discussed.

To demonstrate the kernels, two plots are defined. The first plot simply draws the kernel function as a function of a hypothetical 1D input vector, with the other vector held constant.

```{r}
plot_kernel_value <- function(kernel, ...) {
  tibble::tibble(
    X1 = seq(0, 5, by = 0.05),
    X2 = rep(0, length(X1)),
    kv = purrr::map2_dbl(X1, X2, kernel, ...)
  ) %>%
    ggplot(aes(x = X1, y = kv)) +
    geom_line() +
    labs(x = "1st dimension", y = "kernel value") +
    theme_minimal()
}
```

The next plot samples from the Gaussian process that uses the kernel function to calculate its covariance matrix. This essentially translates to sampling random functions from the Gaussian process. See the post on [Bayesian optimisation](../bayesian-opt-r) for details.

```{r}
#' Random Samples from a Multivariate Gaussian
#' 
#' This implementation is similar to MASS::mvrnorm, but uses chlosky
#' decomposition instead. This should be more stable but is less efficient than
#' the MASS implementation, which recycles the eigen decomposition for the
#' sampling part.
#'
#' @param n number of samples to sample
#' @param mu the mean of each input dimension
#' @param sigma the covariance matrix
#' @param epsilon numerical tolerance added to the diagonal of the covariance
#'  matrix. This is necessary for the Cholesky decomposition, in some cases.
#'
#' @return numerical vector of n samples
rmvnorm <- function(n = 1, mu, sigma, epsilon = 1e-6) {
    p <- length(mu)
    if(!all(dim(sigma) == c(p, p))) stop("incompatible dimensions of arguments")
    ev <- eigen(sigma, symmetric = TRUE)$values
    if(!all(ev >= -epsilon*abs(ev[1L]))) {
      stop("The covariance matrix (sigma) is not positive definite")
    }
    cholesky <- chol(sigma + diag(p)*epsilon)
    sample <- rnorm(p*n, 0, 1)
    dim(sample) <- c(n, p)
    sweep(sample %*% cholesky, 2, mu, FUN = `+`)
}

plot_gp <- function(kernel, ...) {
  n_samples <- 5
  X_predict <- matrix(seq(-5, 5, length.out = 100), 100, 1)
  mu <- rep(0, times = length(X_predict))
  sigma <- rlang::exec(kernel, X1 = X_predict, X2 = X_predict, ...)
  samples <- rmvnorm(n = n_samples, mu, sigma)
  p <- tibble::as_tibble(
    t(samples),
    .name_repair = ~ paste("sample", seq(1, n_samples))
  ) %>%
    dplyr::mutate(
    x = X_predict,
    uncertainty = 1.6*sqrt(diag(sigma)),
    mu = mu,
    lower = mu - uncertainty,
    upper = mu + uncertainty
  ) %>%
    ggplot(aes(x = x)) +
    geom_ribbon(
      aes(ymin = lower, ymax = upper, fill = "89% interval"),
      alpha = 0.2
    ) +
    geom_line(aes(y = mu, colour = "Mean")) +
    theme_minimal() +
    labs(
      y = "y",
      x = "x",
      colour = "",
      fill = ""
    ) +
    theme(panel.grid = element_blank())
  Reduce(
    `+`,
    init = p,
    x = lapply(paste("sample", seq(1, n_samples)), function(s) {
      geom_line(aes(y = .data[[s]], colour = s), linetype = 2)
    })
  ) +
    scale_colour_brewer(palette = "YlGnBu") +
    scale_fill_manual(values = list("89% interval" = "#219ebc"))
}
```

## Stationary Kernels

Stationary kernels are a class of kernel functions that are invariant to translations in the input space. Mathematically, a kernel function is stationary if it depends only on the difference between its arguments, $\mathbf{x}-\mathbf{x'}$, and not on their absolute values. Formally, a kernel function $k(\mathbf{x}, \mathbf{x'})$ is stationary if and only if:

$$k(\mathbf{x}, \mathbf{x'}) = k(\mathbf{x} + a, \mathbf{x'} + a)$$

for all inputs $\mathbf{x}$ and $\mathbf{x'}$ and all $a$.

### RBF Kernel

The Radial Basis Function (RBF) kernel is a popular choice in Gaussian processes. Its definition is

$$k(\mathbf{x}, \mathbf{x'}) = \sigma^2 \exp\left(-\frac{1}{2}\frac{\lVert \mathbf{x} - \mathbf{x'}\rVert^2}{l^2}\right)$$

$\lVert \mathbf{x} - \mathbf{x'}\rVert^2$ is the squared Euclidean distance between the two vectors. $\sigma_f^2$ is a variance parameter that simply scales the kernel. More interestingly, the length scale parameter, $l$, controls the smoothness and the range of influence of the kernel. It determines how quickly the similarity between two input points decreases as their distance increases.

Intuitively, small length scales mean that two points have to be very close to have any correlation. This results in very flexible functions that do not expect much correlation between data points. For a large length scale, however, points that are far apart are still expected to behave in a similar way. This results in very smooth functions that expect similar output values across the entire covariate space.

The flexibility and interpretability of the length scale parameter makes the RBF kernel a good starting point, when exploring Gaussian processes.

```{r}
#' RBF Kernel
#'
#' @param X1 matrix of dimensions (n, d). Vectors are coerced to (1, d).
#' @param X2 matrix of dimensions (m, d). Vectors are coerced to (1, d).
#' @param l length scale
#' @param sigma_f scale parameter 
#'
#' @return matrix of dimensions (n, m)
rbf_kernel <- function(X1, X2, l = 1.0, sigma_f = 1.0) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  sqdist <- (- 2*(X1 %*% t(X2))) %>%
    add(rowSums(X1**2, dims = 1)) %>%
    sweep(2, rowSums(X2**2, dims = 1), `+`)
  sigma_f**2 * exp(-0.5 / l**2 * sqdist)
}
```

Here is an example of of the covariance tapers off between two vectors, as their Euclidean distance increases

```{r}
plot_kernel_value(rbf_kernel)
```

Random functions pulled from a Gaussian process that employs the RBF kernel are quite flexible.

```{r}
plot_gp(rbf_kernel)
```

#### RQ Kernel

The Rational Quadratic (RQ) Kernel is a stationary kernel that is often used in Gaussian process regression to model functions with a mixture of different length scales. It is defined as:

$$k(x, x') = \sigma^2\left(1 + \frac{\Vert x - x'\Vert^2}{2\alpha \ell^2}\right)^{-\alpha}$$

Advantages:

The Rational Quadratic Kernel can model functions with a mixture of different length scales, making it useful for problems where the function may have both local and global variations.
The shape parameter $\alpha$ controls the smoothness of the function, allowing the kernel to model both smooth and non-smooth functions.
The Rational Quadratic Kernel is flexible and can be combined with other kernel functions to create more complex covariance structures.

Pitfalls:

The Rational Quadratic Kernel has an additional hyperparameter, $\alpha$, that needs to be tuned. This can make the kernel more difficult to use in practice.
The Rational Quadratic Kernel can be sensitive to the choice of length scale parameter $\ell$. Choosing an appropriate length scale can be challenging, especially when the input space is high-dimensional.
The Rational Quadratic Kernel can be computationally expensive to evaluate, especially for large datasets, since it involves computing pairwise distances between all pairs of input points.

#### Mat√©rn Kernel

## Periodic Kernels

Periodic kernels are a class of kernel functions that exhibit periodicity in the input space. Mathematically, a periodic kernel function $k(\mathbf{x}, \mathbf{x'})$ is periodic if it depends on the difference between its arguments, $\mathbf{x}-\mathbf{x'}$, as well as their absolute values. Formally, a periodic kernel function $k(\mathbf{x}, \mathbf{x'})$ is periodic if and only if:

$$k(\mathbf{x}, \mathbf{x'}) = k(\mathbf{x}-\mathbf{x'}) = k(\mathbf{x}+n\omega, \mathbf{x'}+n\omega)$$

for all inputs $\mathbf{x}$ and $\mathbf{x'}$ and all integers $n$, where $\omega$ is the period of the kernel.

#### The Basic Periodic Kernel

The basic periodic kernel is defined as:

$$k(x, x') = \exp \left( -\frac{2 \sin^2\left(\pi \frac{\|x-x'\|}{p}\right)}{l^2} \right)$$

Where $p$ is the period and $l$ is the length scale.



```{r}
# Define the Periodic Kernel function
periodic_kernel <- function(X1, X2, sigma2, length_scale, period) {
  # Calculate pairwise distances between input points
  D <- as.matrix(dist(rbind(X1, X2)))
  
  # Calculate the kernel matrix element for each pair of input points
  K <- sigma2 * exp(-2 * sin(pi * D / period)^2 / length_scale^2)
  
  # Return the covariance matrix
  return(K[1:length(X1), (length(X1)+1):(length(X1)+length(X2))])
}

```

Advantages:

The Periodic Kernel can model functions with periodic behavior, such as seasonal trends in time series data or oscillatory patterns in spatial data.
The Periodic Kernel is able to capture long-term periodicity and short-term variations in the function.
The Periodic Kernel can be combined with other kernel functions to model more complex covariance structures.

Pitfalls:

The Periodic Kernel has an additional hyperparameter, the period parameter, that needs to be tuned. The choice of period can have a significant impact on the model performance, and finding an appropriate value can be challenging.
The Periodic Kernel may not be suitable for modeling functions with non-periodic behavior. If the function being modeled does not exhibit periodicity, using the Periodic Kernel may lead to poor model performance.
The Periodic Kernel can be computationally expensive to evaluate, especially for large datasets, since it involves computing pairwise distances between all pairs of input points.

## Other Non-stationary Kernels

Unlike stationary kernels, non-stationary kernels can depend on the absolute values of their inputs, and their correlation function may vary with the location of the inputs.

Non-stationary kernels are often used when the underlying function being modelled exhibits varying behaviour or has different characteristics in different regions of the input space. For example, if the function being modelled changes rapidly in some regions and slowly in others, a non-stationary kernel may be more appropriate than a stationary kernel.

### Linear Kernel

$$ K_L(\boldsymbol{x}, \boldsymbol{x'}) = \boldsymbol{x}^T \boldsymbol{x'} $$

It is appropriate to use the linear kernel in cases where the relationship between the input variables is believed to be roughly linear.

Unlike some other kernel functions, such as the Gaussian kernel or the polynomial kernel, the linear kernel does not have any hyperparameters that need to be set or tuned. This can make it a convenient and efficient choice for certain applications, as it requires less computational resources and does not require extensive parameter tuning.

```{r}
linear_kernel <- function(X1, X2, alpha = 0, sigma = 0) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  sigma^2 + (X1 - alpha) %*% t(X2 - alpha)
}
```

```{r plot-linear-kernel}
plot_kernel_value(linear_kernel, alpha = 1)
```

```{r}
plot_gp(linear_kernel)
```



## Discrete Kernels

Kronecker Delta Kernel: This kernel assigns a value of 1 to pairs of input points that are equal and a value of 0 otherwise. It is often used to model categorical data.

Ordinal Kernel

Histogram Intersection Kernel

Spectral Kernel

Dirichlet Process Kernel

Jaccard kernel

Cosine kernel

Levenshtein kernel

#### Hamming kernel

$$k(x, x') = \frac{1}{L}\sum_{i=1}^{L} [x_i = x'_i]$$

The Hamming Kernel is a discrete kernel function that is commonly used for modeling binary data, such as DNA sequences. It assigns a value of 1 to pairs of input points that have the same binary code and a value of 0 otherwise. The Hamming distance between two binary strings is the number of positions in which they differ.

```{r}
hamming_kernel <- function(x, y) {
  L <- length(x)
  kernel <- sum(x == y) / L
  return(kernel)
}

```


Chi-squared Kernel

$$K(\mathbf{x}, \mathbf{y}) = \exp\left(-\frac{1}{2}\sum_{i=1}^{n} \frac{(x_i-y_i)^2}{x_i+y_i}\right)$$

Intuitively, the chi-squared kernel measures the similarity between two histograms by comparing the differences between their corresponding bins, taking into account the size of each bin. The kernel is normalized so that its values lie between 0 and 1, where a value of 1 indicates identical histograms and a value of 0 indicates completely dissimilar histograms.

```{r}
chi_squared_kernel <- function(x, y) {
  # x: vector of histogram bin counts or probabilities
  # y: vector of histogram bin counts or probabilities
  
  # Check that x and y have the same length
  if (length(x) != length(y)) {
    stop("Input vectors must have the same length")
  }
  
  # Compute chi-squared distance between x and y
  dist <- sum((x - y)^2 / (x + y))
  
  # Compute chi-squared kernel using distance
  kernel <- exp(-0.5 * dist)
  
  return(kernel)
}

```


Lattice Kernel

Product Kernel

## What are anisotropic kernels and how to implement them?

Anisotropic kernels are covariance functions in Gaussian process regression that allow for different levels of smoothness in different directions or dimensions of the input space. In other words, the kernel is "stretched" or "compressed" in certain directions, which can be useful when the input variables have different units or scales.

The anisotropy of a kernel is controlled by one or more length scales, which specify the characteristic length of variations in the input space along each dimension. In isotropic kernels, there is a single length scale that controls the smoothness in all dimensions, whereas in anisotropic kernels there can be multiple length scales, one for each dimension.

Examples of anisotropic kernels include the RBF kernel and the Matern kernel with anisotropic length scales.

The use of anisotropic kernels can improve the predictive accuracy of Gaussian process models when the input variables have different levels of variability or when the correlations between variables vary in different directions. However, anisotropic kernels can also increase the complexity of the model and require more computational resources for optimization and inference.

```{r}
# Anisotropic RBF kernel (vectorized)
aniso_rbf_kernel <- function(X1, X2, gamma, l) {
  # X1: n x p matrix
  # X2: m x p matrix
  # gamma: scalar, the kernel width parameter
  # l: p-dimensional vector, the length scale parameter
  
  n <- nrow(X1)
  m <- nrow(X2)
  
  # Compute the squared Euclidean distance between every pair of points
  D <- -2 * crossprod(X1, X2, sparse = TRUE) +
    rowSums(X1^2) %*% rep(1, m) +
    rep(1, n) %*% t(colSums(X2^2))
  D <- sqrt(max(0, D))
  
  # Compute the kernel matrix using the RBF kernel function
  K <- exp(-gamma * (D^2 / outer(l^2, rep(1, m), "*")))
  
  return(K)
}

```


## Adding Noise to Kernels

## Combining Kernels

Kernels can be combined in several ways to create more powerful and flexible kernel functions. Here are some common ways to combine kernels:

### Combined Homogenous Kernels

Summation: Two or more kernels can be added together to create a new kernel that captures the features of both. For example, if you have two kernels K1 and K2, you can combine them as K = K1 + K2. The resulting kernel K will assign high values to pairs of points that are similar in either K1 or K2.

Product: Two or more kernels can be multiplied together to create a new kernel that captures the features of both. For example, if you have two kernels K1 and K2, you can combine them as K = K1 * K2. The resulting kernel K will assign high values to pairs of points that are similar in both K1 and K2.

Convolution: Two or more kernels can be convolved together to create a new kernel that captures the features of both. For example, if you have two kernels K1 and K2, you can combine them as K = K1 * K2, where * represents convolution. The resulting kernel K will assign high values to pairs of points that are similar in both K1 and K2, but will also capture more complex patterns that cannot be captured by either kernel alone.

Weighted combination: Two or more kernels can be combined as a weighted sum to create a new kernel that assigns different weights to each kernel. For example, if you have two kernels K1 and K2, you can combine them as K = w1 * K1 + w2 * K2, where w1 and w2 are the weights assigned to K1 and K2, respectively. The resulting kernel K will assign high values to pairs of points that are similar in either K1 or K2, but the weights can be adjusted to give more importance to one kernel over the other.

Power: A kernel can be raised to a power to create a new kernel that captures higher-order patterns. For example, if you have a kernel K, you can raise it to the power of p as K = K^p. The resulting kernel K will assign high values to pairs of points that are similar in K, but will also capture higher-order patterns that cannot be captured by K alone.

By combining kernels in different ways, you can create more complex and flexible kernel functions that can capture a wide range of patterns in the input data.

### Combined Heterogenous Kernels

The idea behind using heterogeneous kernels is to allow the model to capture different types of patterns in different dimensions of the input data. For example, if you have a dataset with features that have different types of scales, you might want to use a linear kernel for the features that are on a similar scale and a radial basis function (RBF) kernel for the features that have a different scale. Another example is in natural language processing, where a bag-of-words representation can be combined with a kernel that captures syntactic or semantic relations between words.

```{r}
mixed_kernel <- function(X1, X2, gamma, l, kernel_type) {
  # X1: n x p matrix
  # X2: m x p matrix
  # gamma: scalar, the kernel width parameter
  # l: p-dimensional vector, the length scale parameter
  # kernel_type: p-dimensional vector, specifies the kernel type for each dimension
  
  n <- nrow(X1)
  m <- nrow(X2)
  p <- ncol(X1)
  
  # Initialize kernel matrix to zeros
  K <- matrix(0, n, m)
  
  for (i in 1:p) {
    # Compute kernel matrix for current dimension
    if (kernel_type[i] == "rbf") {
      K_i <- exp(-gamma * ((X1[,i] %*% t(rep(1,m)))^2 + (rep(1,n) %*% t(X2[,i]))^2 - 2*X1[,i] %*% t(X2[,i])) / l[i]^2)
    } else if (kernel_type[i] == "linear") {
      K_i <- X1[,i] %*% t(X2[,i])
    } else {
      stop("Unsupported kernel type")
    }
    
    # Add kernel matrix to mixed kernel matrix
    K <- K + K_i
  }
  
  return(K)
}

```

