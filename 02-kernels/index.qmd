---
title: ""
bibliography: references.bib
csl: ../citation_style.csl
format:
  html:
    fig-width: 8
    fig-height: 5
    theme: default
html-math-method:
  method: katex
---

```{r func, echo=TRUE}
library(ggplot2)
library(magrittr)
set.seed(4444)
```

This post takes an extensive look at kernels and discusses the rationales, applications, and limitations of some popular kernels.

Kernels, also known as covariance functions or kernel functions, are central to Gaussian processes and Bayesian optimisation. They provide the main means of implementing prior knowledge about the modelled process into Bayesian optimisation or any other machine learning procedure that takes advantage of kernels.

In Gaussian processes, kernel functions define the covariance between any two points in a dataset. Intuitively, kernels quantify how similar two points are, given just their position in input space. The kernel function determines the smoothness and complexity of the resulting Gaussian process model, and it controls how much weight is given to different regions of the input space. Different types of kernel functions can be used to model different types of data, such as periodic or spatial data.

Bayesian optimisation, extensively employs Gaussian processes, so kernels provide the main means of define a prior distribution over the objective function being optimised. There are a plethora of kernels available, and for successful implementations of Bayesian optimisation, selecting the right one is essential but difficult.

## Types of Kernels

Formally, a kernel function $k(\mathbf{x},\mathbf{x'})$ takes two inputs, $\mathbf{x}$ and $\mathbf{x'}$, and returns a real-valued scalar that represents a similarity measure between them.

A kernel function must be positive semi-definite (PSD) and symmetric. This means that for any set of $n$ input vectors ${\mathbf{x}_1, \ldots, \mathbf{x}n}$, the kernel matrix $\mathbf{\Sigma}$, defined as $\mathbf{\Sigma}{i,j} = k(\mathbf{x}_i, \mathbf{x}_j)$, must be PSD and symmetric. The PSD property ensures that the kernel matrix can be applied as the covariance matrix in a Gaussian process.

There are several categories of functions that adhere to these general definitions, and some of them add additional conditions.

#### Stationary Kernels

Stationary kernels are a class of kernel functions that are invariant to translations in the input space. Mathematically, a kernel function $k(\mathbf{x}, \mathbf{x'})$ is stationary if it depends only on the difference between its arguments, $\mathbf{x}-\mathbf{x'}$, and not on their absolute values. Formally, a kernel function $k(\mathbf{x}, \mathbf{x'})$ is stationary if and only if:

$$k(\mathbf{x}, \mathbf{x'}) = k(\mathbf{x} + a, \mathbf{x'} + a)$$

for all inputs $\mathbf{x}$ and $\mathbf{x'}$ and all $a$.

Examples include the Radial Basis Function (RBF) kernel and the Rational Quadratic (RQ) kernel.

#### Periodic Kernels

Periodic kernels are a class of kernel functions that exhibit periodicity in the input space. Mathematically, a periodic kernel function $k(\mathbf{x}, \mathbf{x'})$ is periodic if it depends on the difference between its arguments, $\mathbf{x}-\mathbf{x'}$, as well as their absolute values. Formally, a periodic kernel function $k(\mathbf{x}, \mathbf{x'})$ is periodic if and only if:

$$k(\mathbf{x}, \mathbf{x'}) = k(\mathbf{x}-\mathbf{x'}) = k(\mathbf{x}+n\omega, \mathbf{x'}+n\omega)$$

for all inputs $\mathbf{x}$ and $\mathbf{x'}$ and all integers $n$, where $omega$ is the period of the kernel.

An example of a periodic kernel is the locally periodic kernel.

#### Other Non-stationary Kernels

Unlike stationary kernels, non-stationary kernels can depend on the absolute values of their inputs, and their correlation function may vary with the location of the inputs.

Non-stationary kernels are often used when the underlying function being modelled exhibits varying behaviour or has different characteristics in different regions of the input space. For example, if the function being modelled changes rapidly in some regions and slowly in others, a non-stationary kernel may be more appropriate than a stationary kernel.

An example of a non-stationary kernel is the linear kernel.

## Choosing the right kernel

In the context of a Gaussian process that should approximate an objective function, a good kernel function should be flexible enough to capture the underlying structure of the data, but not so flexible that it overfits the data. The choice of kernel function and its hyperparameters can have a significant impact on the performance of the Gaussian process and its application in Bayesian optimisation, so it is important to choose carefully and experiment with different options.

However, without knowing the virtues, pitfalls, and assumptions of a kernel, it is difficult to assess its quality for a given problem. In the following sections, a selection of kernels and their virtues are discussed. 
Before going there, however, two things are needed. First, we need a nice way to plot the kernel function

```{r}
# Define kernel function
my_kernel <- function(x, y) {
  exp(-0.5 * sum((x - y)^2))
}

# Set range of distances to evaluate
distances <- seq(0, 3, by = 0.05)

# Compute kernel values for each distance
kernel_values <- sapply(distances, function(d) my_kernel(c(0, 0), c(d, 0)))

# Combine distances and kernel values into a data frame
df <- data.frame(distance = distances, kernel_value = kernel_values)

# Create plot using ggplot2
ggplot(df, aes(x = distance, y = kernel_value)) + 
  geom_line() + 
  labs(x = "Distance", y = "Kernel Value") +
  theme_minimal()


```



### Linear Kernel

$$ K_L(\boldsymbol{x}, \boldsymbol{x'}) = \boldsymbol{x}^T \boldsymbol{x'} $$

It is appropriate to use the linear kernel in cases where the relationship between the input variables is believed to be roughly linear.

Unlike some other kernel functions, such as the Gaussian kernel or the polynomial kernel, the linear kernel does not have any hyperparameters that need to be set or tuned. This can make it a convenient and efficient choice for certain applications, as it requires less computational resources and does not require extensive parameter tuning.

```{r}
linear_kernel <- function(X1, X2) {
  if (is.vector(X1)) {
    X1 <- matrix(X1, nrow = length(X1), ncol = 1)
  }
  if (is.vector(X2)) {
    X2 <- matrix(X2, nrow = length(X2), ncol = 1)
  }
  return(X1 %*% t(X2))
}
```


### RBF Kernel

$$k(\mathbf{x}_i, \mathbf{x}_j) = \sigma_f^2 \exp\left(-\frac{1}{2}\frac{\lVert \mathbf{x}_i - \mathbf{x}_j\rVert^2}{l^2}\right)$$

The length scale parameter $l$ in the RBF kernel controls the smoothness and the range of influence of the kernel. It determines how quickly the similarity between two input points decreases as their distance increases.

Intuitively, the length scale parameter $l$ can be thought of as the characteristic length scale of the underlying function that generates the data. If the length scale is small, it implies that the function varies rapidly over small distances, and the kernel will be sensitive to small changes in the input. On the other hand, if the length scale is large, it implies that the function varies slowly over large distances, and the kernel will be less sensitive to small changes in the input.

For example, consider the case of modelling a spatial process with the RBF kernel. If the length scale is small, it implies that nearby locations have very different values, and the kernel will assign a low similarity between them. In contrast, if the length scale is large, it implies that nearby locations have similar values, and the kernel will assign a high similarity between them. Similarly, in the case of time series data, a smaller length scale implies that the kernel is more sensitive to rapid changes in the signal, while a larger length scale implies that the kernel is more sensitive to slower trends and patterns.

```{r}
rbf_kernel <- function(X1, X2, l) {
  # Compute pairwise Euclidean distance between X1 and X2
  dist_mat <- as.matrix(dist(rbind(X1, X2)))
  K <- exp(- dist_mat^2 / (2 * l^2))
  
  # Extract the submatrices corresponding to K(X1, X1), K(X1, X2), K(X2, X1), K(X2, X2)
  n1 <- nrow(X1)
  n2 <- nrow(X2)
  K11 <- K[1:n1, 1:n1]
  K12 <- K[1:n1, (n1 + 1):(n1 + n2)]
  K21 <- K[(n1 + 1):(n1 + n2), 1:n1]
  K22 <- K[(n1 + 1):(n1 + n2), (n1 + 1):(n1 + n2)]
  
  # Return the kernel matrix K
  return(K)
}
```

### MatÃ©rn Kernel

$$ k_{\mathrm{Matern}}(r) = \frac{1}{\Gamma(\nu)2^{\nu-1}}\left(\frac{\sqrt{2\nu}r}{l}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu}r}{l}\right) $$

```{r}
matern_kernel <- function(X1, X2, nu, l) {
  # Compute Euclidean distance between each pair of rows
  dist_mat <- as.matrix(dist(rbind(X1, X2)))
  dist_X1X2 <- dist_mat[1:nrow(X1), (nrow(X1)+1):nrow(dist_mat)]

  # Compute the kernel
  sqrt_2nu_over_l <- sqrt(2 * nu) / l
  term1 <- ((2^(1 - nu)) / gamma(nu)) * ((sqrt_2nu_over_l) ^ nu) * dist_X1X2 ^ nu
  term2 <- besselK(sqrt_2nu_over_l * dist_X1X2, nu)
  K <- term1 * term2

  return(K)
}
```

#### Rational Quadratic Kernel

The Rational Quadratic Kernel is a stationary kernel that is often used in Gaussian process regression to model functions with a mixture of different length scales. It is defined as:

$$k_{\text{RQ}}(x, x') = \left(1 + \frac{(x - x')^2}{2\alpha \ell^2}\right)^{-\alpha}$$

Advantages:

The Rational Quadratic Kernel can model functions with a mixture of different length scales, making it useful for problems where the function may have both local and global variations.
The shape parameter $\alpha$ controls the smoothness of the function, allowing the kernel to model both smooth and non-smooth functions.
The Rational Quadratic Kernel is flexible and can be combined with other kernel functions to create more complex covariance structures.

Pitfalls:

The Rational Quadratic Kernel has an additional hyperparameter, $\alpha$, that needs to be tuned. This can make the kernel more difficult to use in practice.
The Rational Quadratic Kernel can be sensitive to the choice of length scale parameter $\ell$. Choosing an appropriate length scale can be challenging, especially when the input space is high-dimensional.
The Rational Quadratic Kernel can be computationally expensive to evaluate, especially for large datasets, since it involves computing pairwise distances between all pairs of input points.



### Periodic Kernels

$$k_{\mathrm{per}}(\mathbf{x}_i, \mathbf{x}_j) = \sigma^2 \exp\left(-\frac{2\sin^2\left(\frac{\pi}{p}\|\mathbf{x}_i-\mathbf{x}_j\|\right)}{\ell^2}\right)$$

```{r}
# Define the Periodic Kernel function
periodic_kernel <- function(X1, X2, sigma2, length_scale, period) {
  # Calculate pairwise distances between input points
  D <- as.matrix(dist(rbind(X1, X2)))
  
  # Calculate the kernel matrix element for each pair of input points
  K <- sigma2 * exp(-2 * sin(pi * D / period)^2 / length_scale^2)
  
  # Return the covariance matrix
  return(K[1:length(X1), (length(X1)+1):(length(X1)+length(X2))])
}

```

Advantages:

The Periodic Kernel can model functions with periodic behavior, such as seasonal trends in time series data or oscillatory patterns in spatial data.
The Periodic Kernel is able to capture long-term periodicity and short-term variations in the function.
The Periodic Kernel can be combined with other kernel functions to model more complex covariance structures.

Pitfalls:

The Periodic Kernel has an additional hyperparameter, the period parameter, that needs to be tuned. The choice of period can have a significant impact on the model performance, and finding an appropriate value can be challenging.
The Periodic Kernel may not be suitable for modeling functions with non-periodic behavior. If the function being modeled does not exhibit periodicity, using the Periodic Kernel may lead to poor model performance.
The Periodic Kernel can be computationally expensive to evaluate, especially for large datasets, since it involves computing pairwise distances between all pairs of input points.

### Discrete Kernels

Kronecker Delta Kernel: This kernel assigns a value of 1 to pairs of input points that are equal and a value of 0 otherwise. It is often used to model categorical data.

Ordinal Kernel

Histogram Intersection Kernel

Spectral Kernel

Dirichlet Process Kernel

Jaccard kernel

Cosine kernel

Levenshtein kernel

#### Hamming kernel

$$k(x, x') = \frac{1}{L}\sum_{i=1}^{L} [x_i = x'_i]$$

The Hamming Kernel is a discrete kernel function that is commonly used for modeling binary data, such as DNA sequences. It assigns a value of 1 to pairs of input points that have the same binary code and a value of 0 otherwise. The Hamming distance between two binary strings is the number of positions in which they differ.

```{r}
hamming_kernel <- function(x, y) {
  L <- length(x)
  kernel <- sum(x == y) / L
  return(kernel)
}

```


Chi-squared Kernel

$$K(\mathbf{x}, \mathbf{y}) = \exp\left(-\frac{1}{2}\sum_{i=1}^{n} \frac{(x_i-y_i)^2}{x_i+y_i}\right)$$

Intuitively, the chi-squared kernel measures the similarity between two histograms by comparing the differences between their corresponding bins, taking into account the size of each bin. The kernel is normalized so that its values lie between 0 and 1, where a value of 1 indicates identical histograms and a value of 0 indicates completely dissimilar histograms.

```{r}
chi_squared_kernel <- function(x, y) {
  # x: vector of histogram bin counts or probabilities
  # y: vector of histogram bin counts or probabilities
  
  # Check that x and y have the same length
  if (length(x) != length(y)) {
    stop("Input vectors must have the same length")
  }
  
  # Compute chi-squared distance between x and y
  dist <- sum((x - y)^2 / (x + y))
  
  # Compute chi-squared kernel using distance
  kernel <- exp(-0.5 * dist)
  
  return(kernel)
}

```


Lattice Kernel

Product Kernel

## What are anisotropic kernels and how to implement them?

Anisotropic kernels are covariance functions in Gaussian process regression that allow for different levels of smoothness in different directions or dimensions of the input space. In other words, the kernel is "stretched" or "compressed" in certain directions, which can be useful when the input variables have different units or scales.

The anisotropy of a kernel is controlled by one or more length scales, which specify the characteristic length of variations in the input space along each dimension. In isotropic kernels, there is a single length scale that controls the smoothness in all dimensions, whereas in anisotropic kernels there can be multiple length scales, one for each dimension.

Examples of anisotropic kernels include the RBF kernel and the Matern kernel with anisotropic length scales.

The use of anisotropic kernels can improve the predictive accuracy of Gaussian process models when the input variables have different levels of variability or when the correlations between variables vary in different directions. However, anisotropic kernels can also increase the complexity of the model and require more computational resources for optimization and inference.

```{r}
# Anisotropic RBF kernel (vectorized)
aniso_rbf_kernel <- function(X1, X2, gamma, l) {
  # X1: n x p matrix
  # X2: m x p matrix
  # gamma: scalar, the kernel width parameter
  # l: p-dimensional vector, the length scale parameter
  
  n <- nrow(X1)
  m <- nrow(X2)
  
  # Compute the squared Euclidean distance between every pair of points
  D <- -2 * crossprod(X1, X2, sparse = TRUE) +
    rowSums(X1^2) %*% rep(1, m) +
    rep(1, n) %*% t(colSums(X2^2))
  D <- sqrt(max(0, D))
  
  # Compute the kernel matrix using the RBF kernel function
  K <- exp(-gamma * (D^2 / outer(l^2, rep(1, m), "*")))
  
  return(K)
}

```


## Adding Noise to Kernels

## Combining Kernels

Kernels can be combined in several ways to create more powerful and flexible kernel functions. Here are some common ways to combine kernels:

### Combined Homogenous Kernels

Summation: Two or more kernels can be added together to create a new kernel that captures the features of both. For example, if you have two kernels K1 and K2, you can combine them as K = K1 + K2. The resulting kernel K will assign high values to pairs of points that are similar in either K1 or K2.

Product: Two or more kernels can be multiplied together to create a new kernel that captures the features of both. For example, if you have two kernels K1 and K2, you can combine them as K = K1 * K2. The resulting kernel K will assign high values to pairs of points that are similar in both K1 and K2.

Convolution: Two or more kernels can be convolved together to create a new kernel that captures the features of both. For example, if you have two kernels K1 and K2, you can combine them as K = K1 * K2, where * represents convolution. The resulting kernel K will assign high values to pairs of points that are similar in both K1 and K2, but will also capture more complex patterns that cannot be captured by either kernel alone.

Weighted combination: Two or more kernels can be combined as a weighted sum to create a new kernel that assigns different weights to each kernel. For example, if you have two kernels K1 and K2, you can combine them as K = w1 * K1 + w2 * K2, where w1 and w2 are the weights assigned to K1 and K2, respectively. The resulting kernel K will assign high values to pairs of points that are similar in either K1 or K2, but the weights can be adjusted to give more importance to one kernel over the other.

Power: A kernel can be raised to a power to create a new kernel that captures higher-order patterns. For example, if you have a kernel K, you can raise it to the power of p as K = K^p. The resulting kernel K will assign high values to pairs of points that are similar in K, but will also capture higher-order patterns that cannot be captured by K alone.

By combining kernels in different ways, you can create more complex and flexible kernel functions that can capture a wide range of patterns in the input data.

### Combined Heterogenous Kernels

The idea behind using heterogeneous kernels is to allow the model to capture different types of patterns in different dimensions of the input data. For example, if you have a dataset with features that have different types of scales, you might want to use a linear kernel for the features that are on a similar scale and a radial basis function (RBF) kernel for the features that have a different scale. Another example is in natural language processing, where a bag-of-words representation can be combined with a kernel that captures syntactic or semantic relations between words.

```{r}
mixed_kernel <- function(X1, X2, gamma, l, kernel_type) {
  # X1: n x p matrix
  # X2: m x p matrix
  # gamma: scalar, the kernel width parameter
  # l: p-dimensional vector, the length scale parameter
  # kernel_type: p-dimensional vector, specifies the kernel type for each dimension
  
  n <- nrow(X1)
  m <- nrow(X2)
  p <- ncol(X1)
  
  # Initialize kernel matrix to zeros
  K <- matrix(0, n, m)
  
  for (i in 1:p) {
    # Compute kernel matrix for current dimension
    if (kernel_type[i] == "rbf") {
      K_i <- exp(-gamma * ((X1[,i] %*% t(rep(1,m)))^2 + (rep(1,n) %*% t(X2[,i]))^2 - 2*X1[,i] %*% t(X2[,i])) / l[i]^2)
    } else if (kernel_type[i] == "linear") {
      K_i <- X1[,i] %*% t(X2[,i])
    } else {
      stop("Unsupported kernel type")
    }
    
    # Add kernel matrix to mixed kernel matrix
    K <- K + K_i
  }
  
  return(K)
}

```

