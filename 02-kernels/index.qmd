---
title: ""
bibliography: references.bib
csl: ../citation_style.csl
format:
  html:
    fig-width: 8
    fig-height: 5
    theme: default
html-math-method:
  method: katex
---

This post takes an extensive look at kernels and discusses the rationales, applications, and limitations of some popular kernels. Along with the discussion are implementations of many of the kernels with R and the Tidyverse.

```{r func, echo=TRUE}
library(ggplot2)
library(magrittr)
set.seed(4444)
```

Kernels, also known as covariance functions, are central to Gaussian processes and other machine learning methods, where they provide the main means of implementing prior knowledge about the modelled process.

Intuitively, kernels quantify how similar two points are, given just their position in input space. The kernel function determines the smoothness and complexity of the resulting Gaussian process model, and it controls how much weight is given to different regions of the input space. Different types of kernel functions can be used to model different types of data, such as periodic or spatial data.

Bayesian optimisation extensively employs Gaussian processes, so kernels provide the means to define a prior distribution over the objective function being optimised. There are a plethora of kernels available, and for successful implementations of Bayesian optimisation, selecting the right one is essential but difficult.

## Applying Kernels in Gaussian Processes

Formally, a kernel function $k(\mathbf{x},\mathbf{x'})$ takes two inputs, $\mathbf{x}$ and $\mathbf{x'}$, and returns a real-valued scalar that represents a similarity measure between the inputs.

A kernel function must be positive semi-definite (PSD). This means that the kernel matrix, $\mathbf{\Sigma}$, constructed from any set of $n$ input row vectors ${\mathbf{x}_1, \ldots, \mathbf{x}_n}$, must be PSD. The entries of the kernel matrix are defined as $\mathbf{\Sigma}_{i,j} = k(\mathbf{x}_i, \mathbf{x}_j)$, for all combinations of $i, j \in (1, \ldots, n)$. The PSD property ensures that the kernel matrix can be applied as the covariance matrix in a Gaussian process.

In the context of a Gaussian process that should approximate an objective function, a good kernel function should be flexible enough to capture the underlying structure of the data, but not so flexible that it overfits the data. The choice of kernel function and its hyperparameters can have a significant impact on the performance of the Gaussian process and its application in Bayesian optimisation, so it is important to choose carefully and experiment with different options.

However, without knowing the virtues, pitfalls, and assumptions of a kernel, it is difficult to assess its quality for a given problem. In the following sections, a selection of kernels and their virtues are discussed.

To demonstrate the kernels, two plots are defined. The first plot simply draws the kernel function as a function of a hypothetical 1D input vector, with the other vector held constant.

```{r}
plot_kernel_value <- function(kernel, ...) {
  tibble::tibble(
    X1 = seq(0, 5, by = 0.05),
    X2 = rep(0, length(X1)),
    kv = purrr::map2_dbl(X1, X2, kernel, ...)
  ) %>%
    ggplot(aes(x = X1, y = kv)) +
    geom_line() +
    labs(x = "1st dimension", y = "kernel value") +
    theme_minimal()
}
```

The next plot samples from the Gaussian process that uses the kernel function to calculate its covariance matrix. This essentially translates to sampling random functions from the Gaussian process. See the post on [Bayesian optimisation](../bayesian-opt-r) for details.

```{r}
#' Random Samples from a Multivariate Gaussian
#' 
#' This implementation is similar to MASS::mvrnorm, but uses chlosky
#' decomposition instead. This should be more stable but is less efficient than
#' the MASS implementation, which recycles the eigen decomposition for the
#' sampling part.
#'
#' @param n number of samples to sample
#' @param mu the mean of each input dimension
#' @param sigma the covariance matrix
#' @param epsilon numerical tolerance added to the diagonal of the covariance
#'  matrix. This is necessary for the Cholesky decomposition, in some cases.
#'
#' @return numerical vector of n samples
rmvnorm <- function(n = 1, mu, sigma, epsilon = 1e-6) {
    p <- length(mu)
    if(!all(dim(sigma) == c(p, p))) stop("incompatible dimensions of arguments")
    ev <- eigen(sigma, symmetric = TRUE)$values
    if(!all(ev >= -epsilon*abs(ev[1L]))) {
      stop("The covariance matrix (sigma) is not positive definite")
    }
    cholesky <- chol(sigma + diag(p)*epsilon)
    sample <- rnorm(p*n, 0, 1)
    dim(sample) <- c(n, p)
    sweep(sample %*% cholesky, 2, mu, FUN = `+`)
}

plot_gp <- function(kernel, ...) {
  n_samples <- 5
  X_predict <- matrix(seq(-5, 5, length.out = 100), 100, 1)
  mu <- rep(0, times = length(X_predict))
  sigma <- rlang::exec(kernel, X1 = X_predict, X2 = X_predict, ...)
  samples <- rmvnorm(n = n_samples, mu, sigma)
  p <- tibble::as_tibble(
    t(samples),
    .name_repair = ~ paste("sample", seq(1, n_samples))
  ) %>%
    dplyr::mutate(
    x = X_predict,
    uncertainty = 1.6*sqrt(diag(sigma)),
    mu = mu,
    lower = mu - uncertainty,
    upper = mu + uncertainty
  ) %>%
    ggplot(aes(x = x)) +
    geom_ribbon(
      aes(ymin = lower, ymax = upper, fill = "89% interval"),
      alpha = 0.2
    ) +
    geom_line(aes(y = mu, colour = "Mean")) +
    theme_minimal() +
    labs(
      y = "y",
      x = "x",
      colour = "",
      fill = ""
    ) +
    theme(panel.grid = element_blank())
  Reduce(
    `+`,
    init = p,
    x = lapply(paste("sample", seq(1, n_samples)), function(s) {
      geom_line(aes(y = .data[[s]], colour = s), linetype = 2)
    })
  ) +
    scale_colour_brewer(palette = "YlGnBu") +
    scale_fill_manual(values = list("89% interval" = "#219ebc"))
}
```

## Stationary Kernels

Stationary kernels are a class of kernel functions that are invariant to translations in the input space. Mathematically, a kernel function is stationary if it depends only on the difference between its arguments, $\mathbf{x}-\mathbf{x'}$, and not on their absolute values. Formally, a kernel function $k(\mathbf{x}, \mathbf{x'})$ is stationary if and only if:

$$k(\mathbf{x}, \mathbf{x'}) = k(\mathbf{x} + a, \mathbf{x'} + a)$$

for all inputs $\mathbf{x}$ and $\mathbf{x'}$ and all $a$.

### RBF Kernel

The Radial Basis Function (RBF) kernel is also known as the Squared Exponential kernel or the Gaussian kernel. It is a popular choice in Gaussian processes because of its simplicity and interpretability. The RBF kernel is defined as

$$k(\mathbf{x}, \mathbf{x'}) = \sigma^2 \exp\left(-\frac{\lVert \mathbf{x} - \mathbf{x'}\rVert^2}{2l^2}\right)$$

where $\lVert \mathbf{x} - \mathbf{x'}\rVert^2$ is the squared Euclidean distance between the two vectors. $\sigma^2$ is a variance parameter that simply scales the kernel. More interestingly, the length scale parameter, $l$, controls the smoothness and the range of influence of the kernel. It determines how quickly the similarity between two input points decreases as their distance increases.

Intuitively, small length scales mean that two points have to be very close to have any correlation. This results in very flexible functions that do not expect much correlation between data points. For a large length scale, however, points that are far apart are still expected to behave in a similar way. This results in very smooth functions that expect similar output values across the entire covariate space.

The flexibility and interpretability of the length scale parameter makes the RBF kernel a good starting point, when exploring Gaussian processes.

```{r}
#' RBF Kernel
#'
#' @param X1 matrix of dimensions (n, d). Vectors are coerced to (1, d).
#' @param X2 matrix of dimensions (m, d). Vectors are coerced to (1, d).
#' @param l length scale
#' @param sigma scale parameter 
#'
#' @return matrix of dimensions (n, m)
rbf_kernel <- function(X1, X2, l = 1.0, sigma = 1.0) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  sqdist <- (- 2*(X1 %*% t(X2))) %>%
    add(rowSums(X1**2, dims = 1)) %>%
    sweep(2, rowSums(X2**2, dims = 1), `+`)
  sigma**2 * exp(-0.5 / l**2 * sqdist)
}
```

Here is an example of of the covariance tapers off between two vectors, as their Euclidean distance increases

```{r}
plot_kernel_value(rbf_kernel)
```

Random functions pulled from a Gaussian process that employs the RBF kernel are quite flexible.

```{r}
plot_gp(rbf_kernel)
```

#### RQ Kernel

The Rational Quadratic (RQ) Kernel is a generalisation of the RBF kernel in the sense that it can be interpreted as an infinite sum of RBF kernels with different length scales. The RQ kernel is defined as:

$$k(x, x') = \sigma^2\left(1 + \frac{\Vert x - x'\Vert^2}{2\alpha \ell^2}\right)^{-\alpha}$$

where $\lVert \mathbf{x} - \mathbf{x'}\rVert^2$ is the squared Euclidean distance between the two vectors. $\sigma_f^2$ is a variance parameter that simply scales the kernel. The length scale parameter, $l$, determines how quickly the similarity between two input points decreases as their distance increases, just like for the RBF kernel. The mixture parameter, $\alpha$, can be viewed as controlling how much local variation the kernel allows. When drawing functions from a Gaussian process that employs the RQ kernel, small values of $\alpha$ will yield functions with more local variation while still displaying the overall length scaling defined by $l$. On the other hand, larger values of $\alpha$ will yield functions with less local variation. In fact as $\alpha \to \infty$ the RQ kernel converges to the RBF kernel with the same $l$.

Since the RQ Kernel can model functions with a mixture of different length scales, it is useful for problems where the function may have both local and global variations. However, the RQ kernel needs tuning of two hyperparameters, $\alpha$ and $l$, which in turn requires more data. The addition of $\alpha$ also arguably decreases the interpretability of $l$.

The added flexibility compared to the RBF kernel without too much added complexity, makes the RQ kernel a good alternative, when exploring Gaussian processes.

```{r}
rq_kernel <- function(X1, X2, sigma = 1, l = 1, alpha = 1) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  sqdist <- (- 2*(X1 %*% t(X2))) %>%
    add(rowSums(X1**2, dims = 1)) %>%
    sweep(2, rowSums(X2**2, dims = 1), `+`)
  sigma^2 * (1 + sqdist / (2 * alpha * l^2))^(-alpha)
}
```

Here is an example of of the covariance tapers off between two vectors, as their Euclidean distance increases. Notice how much more local variation is allowed

```{r}
plot_kernel_value(rq_kernel, alpha = 0.5, l = 0.5)
```

The emphasis on local variation yields functions which are much more flexible:

```{r}
plot_gp(rq_kernel, alpha = 0.5, l = 0.5)
```

#### Exponential Kernel

The RBF and RQ kernels represent smooth kernels, i.e. kernels that are differentiable and, when applied in Gaussian processes, yield functions that are less prone to abrupt changes. The exponential kernel, on the other hand, is not differentiable. The exponential kernel is defined as

$$k(\mathbf{x}, \mathbf{x'}) = \sigma^2 \exp\left(-\frac{\lVert \mathbf{x} - \mathbf{x'}\rVert}{l}\right)$$

where $\lVert \mathbf{x} - \mathbf{x'}\rVert^2$ is the squared Euclidean distance between the two vectors. $\sigma_f^2$ is a variance parameter that simply scales the kernel. The length scale parameter, $l$, determines how quickly the similarity between two input points decreases as their distance increases, just like for the RBF kernel.

When applied in Gaussian processes, the exponential kernel yields functions that are much less smooth compared to the RBF kernel. This is useful when trying to model functions that exhibit abrupt changes.

```{r}
exponential_kernel <- function(X1, X2, l = 1, sigma = 1) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  distance <- (- 2*(X1 %*% t(X2))) %>%
    add(rowSums(X1**2, dims = 1)) %>%
    sweep(2, rowSums(X2**2, dims = 1), `+`) %>%
    sqrt()
  sigma^2 * exp(-distance / l)
}
```

Here is an example of of the covariance tapers off between two vectors, as their Euclidean distance increases. Notice the abrupt decline in covariance.

```{r}
plot_kernel_value(exponential_kernel, l = 1)
```

The Gaussian process yields functions which are prone to abrupt changes and thus look very rough

```{r}
plot_gp(exponential_kernel, l = 5)
```

#### Matérn Kernel

The Matérn kernel is a flexible and versatile stationary kernel that can model a wide range of functions. The Matérn kernel is given by:

$$k(\mathbf{x},\mathbf{x'}) = \sigma^2\frac{2^{1-\nu}}{\Gamma(v)}\left(\frac{\sqrt{2v}\lVert\mathbf{x}-\mathbf{x'}\rVert}{l}\right)^{\nu} K_{\nu}\left(\frac{\sqrt{2\nu}\lVert\mathbf{x}-\mathbf{x'}\rVert}{l}\right)$$

where $\sigma^2$ is a variance parameter that scales the kernel, $l$ is the length scale parameter, $\nu$ is the smoothness parameter, $\lVert\mathbf{x}-\mathbf{x'}\rVert$ is the distance between the two vectors, $\Gamma(\cdot)$ is the gamma function, and $K_v(\cdot)$ is a modified Bessel function of the second kind with order $\nu$.

The RBF kernel yields smooth functions when applied in a Gaussian process, and the exponential kernel yields rugged functions. The Matérn kernel is a generalisation of the RBF kernel, where the parameter $\nu$ controls the differentiability, and thus smoothness, of the kernel. In fact, setting $\nu = 0.5$ results in the exponential kernel, and as $\nu \to \infty$ the Matérn kernel converges to the RBF kernel.

The Matérn kernel is a popular choice for Gaussian processes, as it takes very weak assumptions about the function being modelled. The length scale and smoothness parameters allow for modelling smooth functions with long-range covariance as well as functions with abrupt changes. The downside is that the kernel is very flexible and it takes data and effort to avoid overfitting.

Base R includes functions for the gamma function as well as a Bessel function of the second kind with given order.

```{r}
matern_kernel <- function(X1, X2, nu = 2.5, l = 1, sigma = 1) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  distance <- (- 2*(X1 %*% t(X2))) %>%
    add(rowSums(X1**2, dims = 1)) %>%
    sweep(2, rowSums(X2**2, dims = 1), `+`) %>%
    sqrt()
  term <- sqrt(2 * nu) * distance / l
  K <- sigma * (2^(1 - nu) / gamma(nu)) * (term^nu) * besselK(term, nu)
  K[distance == 0] <- 1
  K
}
```

The kernel value looks like the RBF kernel or the exponential kernel, depending on the smoothness parameter

```{r}
plot_kernel_value(matern_kernel, nu = 100)
```

The Matérn kernel can yield functions that strike a balance between smoothness and flexibility

```{r}
plot_gp(matern_kernel, nu = 1.5)
```

#### Constant Kernl

The constant kernel, also known as the bias kernel, is a simple kernel defined as

$$k(\mathbf{x}, \mathbf{x'}) = c$$

where $c$ is a constant value.

The constant kernel is characterised by its simplicity, as it assumes a constant function, meaning the output is the same for all input points. The constant kernel can be used alone or combined with other kernels to model more complex relationships between data points. In Gaussian process regression, this kernel can capture constant offsets in the function being learned.

The kernel is mostly useful in combination with other kernels, where it adds an offset or bias to the kernel value.

```{r}
constant_kernel <- function(X1, X2, c = 1) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  matrix(c, dim(X1)[[1]], dim(X2)[[1]])
}
```

The constant kernel function is just a flat, two-dimensional surface parallel to the input space. The height of the surface is equal to the constant value $c$.

```{r}
plot_kernel_value(constant_kernel)
```

Sampled functions from a Gaussian process that utilises the constant kernel are just constant functions. The functions are flat, horizontal lines parallel to the x-axis. There is still variance - each function has a different constant value, determined variance specified by the kernel. If the Gaussian process has a zero mean function, the sampled functions will be horizontal lines with y-values centred around zero, and their heights will be determined by the constant kernel value according to $y = \beta$ where $\beta \sim \mathcal{N}(0, c)$.

```{r}
plot_gp(constant_kernel)
```

## Periodic Kernels

Periodic kernels are a class of kernel functions that exhibit periodicity. Mathematically, a periodic kernel function $k(\mathbf{x}, \mathbf{x'})$ is periodic if it depends on the difference between its arguments, $\mathbf{x}-\mathbf{x'}$, as well as their absolute values. Formally, a periodic kernel function $k(\mathbf{x}, \mathbf{x'})$ is periodic if and only if:

$$k(\mathbf{x}, \mathbf{x'}) = k(\mathbf{x}+n\omega, \mathbf{x'}+n\omega)$$

for all inputs $\mathbf{x}$ and $\mathbf{x'}$ and all integers $n$, where $\omega$ is the period of the kernel.

#### The Basic Periodic Kernel

The basic periodic kernel is a simple periodic kernel that can be used to model functions that exhibit periodic behaviour, i.e., functions that repeat their values in regular intervals. This especially useful when dealing with time series data or spatial data that exhibit cyclical patterns. The rationale behind the basic periodic kernel is to implement a simple periodic component into a kernel function, while complying with the PSD requirement of a kernel function [@MacKay:1998].

The basic periodic kernel is somewhat similar to the RBF kernel with an additional trigonometric term that introduces periodicity. The definition is:

$$k(\mathbf{x}, \mathbf{x'}) = \exp \left( -\frac{2 \sin^2\left(\pi \frac{\lVert\mathbf{x}-\mathbf{x'}\rVert}{\omega}\right)}{l^2} \right)$$

Where $\lVert\mathbf{x}-\mathbf{x'}\rVert$ is the distance between the two vectors, $\omega$ is the period, and $l$ is the length scale.

```{r}
# Define the Periodic Kernel function
periodic_kernel <- function(X1, X2, l = 1, omega = 1, sigma = 1) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  distance <- (- 2*(X1 %*% t(X2))) %>%
    add(rowSums(X1**2, dims = 1)) %>%
    sweep(2, rowSums(X2**2, dims = 1), `+`) %>%
    sqrt()
  sigma * exp(-2 * sin(pi * distance / omega)^2 / l^2)
}
```

Plotting the basic periodic kernel function results in a periodic pattern with peaks and valleys, where the peaks occur when the inputs are multiples of the period, $\omega$ apart. The covariance between input points is highest when the points are separated by multiples of the period length and decreases as the distance between the points deviates from multiples of the period.

```{r}
plot_kernel_value(periodic_kernel, l = 0.5, omega = 1.5)
```

Sampled functions from a Gaussian process that utilises the basic periodic kernel exhibit periodic behaviour. Such functions, when plotted, appear as smooth, oscillating curves that repeat their patterns over regular intervals.

The properties of the basic periodic kernel, i.e. the period $\omega$ and the length scale $l$, determine the characteristics of the sampled functions.

The period, $\omega$ controls the distance between repetitions of the pattern in the sampled functions. A smaller period length will result in more frequent repetitions, while a larger period length will cause the repetitions to be spaced further apart.

The length scale, $l$, determines how smooth or wiggly the sampled functions are, just like it does for the RBF kernel. A smaller length scale will produce more wiggly functions, while a larger length scale will yield smoother functions with fewer oscillations within each period.

```{r}
plot_gp(periodic_kernel, l = 0.5, omega = 1.5)
```

The basic periodic kernel alone cannot capture trends or non-periodic variations in the data, but can be combined with other kernels to create a very powerful model for objective functions with periodicity.

However, compared to simpler kernels, the basic periodic kernel has an additional hyperparameter, the period parameter $\omega$, that needs to be tuned. The choice of period can have a significant impact on the model performance, and finding an appropriate value can be challenging. As with any periodic kernel, applying the basic periodic kernel comes with the assumption that the underlying objective function being modelled exhibits periodic behaviour. If the assumption does not hold true, applying this kernel could be detrimental.

#### Locally Periodic Kernel

An example of the basic periodic kernel working in tandem with a non-periodic kernel is the locally periodic kernel, which is the product of the basic periodic kernel and the RBF kernel.

$$k(\mathbf{x}, \mathbf{x'}) = \sigma^2\exp \left( -\frac{2 \sin^2\left(\pi \frac{\lVert\mathbf{x}-\mathbf{x'}\rVert}{\omega}\right)}{l_p^2} \right) \exp \left( -\frac{\lVert\mathbf{x}-\mathbf{x'}\rVert^2}{2 l_v^2} \right)$$

```{r}
locally_periodic_kernel <- function(X1,
                                    X2,
                                    l_period = 1,
                                    l_var = 1,
                                    omega = 1,
                                    sigma = 1) {
  periodic_kernel(X1, X2, l = l_period, omega = omega, sigma = sigma) *
    rbf_kernel(X1, X2, l = l_period, sigma = 1)
}
```

Plotting the locally periodic kernel results a series of peaks, reflecting the periodic behaviour captured by the periodic kernel. The peaks are highest when the Euclidean distance between the input vectors is a multiple of the period length. The periodic effect is dampened over longer distances due to the length scaling of the RBF kernel.

```{r}
plot_kernel_value(locally_periodic_kernel, omega = 0.5)
```

Sample functions from a Gaussian process that utilises the locally periodic kernel exhibit both periodic behaviour and local variations. The functions appear as smooth, oscillating curves that repeat their patterns over regular intervals, but with varying amplitude and local changes.

```{r}
plot_gp(locally_periodic_kernel, omega = 0.5)
```

The locally periodic kernel could be combined with other non-periodic kernel like the Matérn kernel to create even more complex dynamics. However, applying a periodic kernel comes with the assumption that the underlying objective function being modelled exhibits periodic behaviour. If the assumption does not hold true, applying a periodic kernel could be detrimental.

## Other Non-stationary Kernels

Unlike stationary kernels, non-stationary kernels can depend on the absolute values of their inputs, and their correlation function may vary with the location of the inputs.

Non-stationary kernels are often used when the underlying function being modelled exhibits varying behaviour or has different characteristics in different regions of the input space. For example, if the function being modelled changes rapidly in some regions and slowly in others, a non-stationary kernel may be more appropriate than a stationary kernel.

### Linear Kernel

The linear kernel, also known as the dot product kernel, is a simple non-stationary kernel function.

$$ K(\mathbf{x}, \mathbf{x'}) = \alpha\mathbf{x}^T \mathbf{x'} + \beta $$

It is appropriate to use the linear kernel in cases where the relationship between the input variables is believed to be roughly linear.

Unlike some other kernel functions, such as the Gaussian kernel or the polynomial kernel, the linear kernel does not have any hyperparameters that need to be set or tuned. This can make it a convenient and efficient choice for certain applications, as it requires less computational resources and does not require extensive parameter tuning.

```{r}
linear_kernel <- function(X1, X2, c = 0) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  c + X1 %*% t(X2)
}
```

```{r}
plot_gp(linear_kernel)
```


```{r}
polynomial_kernel <- function(X1, X2, c = 0, nu = 2) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  (c + X1 %*% t(X2))^nu
}
```

```{r}
plot_gp(polynomial_kernel, c = 1, nu = 2)
```

#### Gibbs Kernel

The RBF kernel has a single parameter that determines the length scale across the entire function. What if that length scale were a function of the input vectors as well?

The Gibbs kernel is a generalisation of the RBF kernel where the length scale is a function of the input vectors rather than a scalar [Paciorek:2003].

$$k(x, x') = \sqrt{\frac{l(x)l(x')}{l^2(x) + l^2(x')}} \exp\left(-\frac{(x - x')^2}{l^2(x) + l^2(x')}\right)$$

$$k(x, x') = \prod_{i = 1}^d\sqrt{\frac{2\ell(x)\ell(x')}{\ell^2(x) + \ell^2(x')}}\mathrm{exp}\left[ -\frac{(x_i - x_i')^2}{\ell^2(x) + \ell^2(x')} \right]
$$

```{r}
# Define the linear length scale function
length_scale_function <- function(X, a, b) {
  a + b * X
}

# Define the Gibbs kernel
gibbs_kernel <- function(X1, X2, l ,...) {
  lx1 <- l(X1, ...)
  lx2 <- l(X2, ...)
  
  diff_squared <- outer(X1, X2, FUN = function(x, y) (x - y)^2)
  lx1_lx2 <- outer(lx1, lx2)
  lx1_sq_plus_lx2_sq <- outer(lx1^2, lx2^2, FUN = "+")
  
  K <- sqrt(lx1_lx2 / lx1_sq_plus_lx2_sq) * exp(-diff_squared / lx1_sq_plus_lx2_sq)
  
  return(K)
}
```


## Discrete Kernels

Kronecker Delta Kernel: This kernel assigns a value of 1 to pairs of input points that are equal and a value of 0 otherwise. It is often used to model categorical data.

Ordinal Kernel

Histogram Intersection Kernel

Spectral Kernel

Dirichlet Process Kernel

Jaccard kernel

Cosine kernel

Levenshtein kernel

#### Hamming kernel

$$k(x, x') = \frac{1}{L}\sum_{i=1}^{L} [x_i = x'_i]$$

The Hamming Kernel is a discrete kernel function that is commonly used for modeling binary data, such as DNA sequences. It assigns a value of 1 to pairs of input points that have the same binary code and a value of 0 otherwise. The Hamming distance between two binary strings is the number of positions in which they differ.

```{r}
hamming_kernel <- function(x, y) {
  L <- length(x)
  kernel <- sum(x == y) / L
  return(kernel)
}

```


Chi-squared Kernel

$$K(\mathbf{x}, \mathbf{y}) = \exp\left(-\frac{1}{2}\sum_{i=1}^{n} \frac{(x_i-y_i)^2}{x_i+y_i}\right)$$

Intuitively, the chi-squared kernel measures the similarity between two histograms by comparing the differences between their corresponding bins, taking into account the size of each bin. The kernel is normalized so that its values lie between 0 and 1, where a value of 1 indicates identical histograms and a value of 0 indicates completely dissimilar histograms.

```{r}
chi_squared_kernel <- function(x, y) {
  # x: vector of histogram bin counts or probabilities
  # y: vector of histogram bin counts or probabilities
  
  # Check that x and y have the same length
  if (length(x) != length(y)) {
    stop("Input vectors must have the same length")
  }
  
  # Compute chi-squared distance between x and y
  dist <- sum((x - y)^2 / (x + y))
  
  # Compute chi-squared kernel using distance
  kernel <- exp(-0.5 * dist)
  
  return(kernel)
}

```


Lattice Kernel

Product Kernel

## What are anisotropic kernels and how to implement them?

Anisotropic kernels are covariance functions in Gaussian process regression that allow for different levels of smoothness in different directions or dimensions of the input space. In other words, the kernel is "stretched" or "compressed" in certain directions, which can be useful when the input variables have different units or scales.

The anisotropy of a kernel is controlled by one or more length scales, which specify the characteristic length of variations in the input space along each dimension. In isotropic kernels, there is a single length scale that controls the smoothness in all dimensions, whereas in anisotropic kernels there can be multiple length scales, one for each dimension.

Examples of anisotropic kernels include the RBF kernel and the Matern kernel with anisotropic length scales.

The use of anisotropic kernels can improve the predictive accuracy of Gaussian process models when the input variables have different levels of variability or when the correlations between variables vary in different directions. However, anisotropic kernels can also increase the complexity of the model and require more computational resources for optimization and inference.

```{r}
# Anisotropic RBF kernel (vectorized)
aniso_rbf_kernel <- function(X1, X2, gamma, l) {
  # X1: n x p matrix
  # X2: m x p matrix
  # gamma: scalar, the kernel width parameter
  # l: p-dimensional vector, the length scale parameter
  
  n <- nrow(X1)
  m <- nrow(X2)
  
  # Compute the squared Euclidean distance between every pair of points
  D <- -2 * crossprod(X1, X2, sparse = TRUE) +
    rowSums(X1^2) %*% rep(1, m) +
    rep(1, n) %*% t(colSums(X2^2))
  D <- sqrt(max(0, D))
  
  # Compute the kernel matrix using the RBF kernel function
  K <- exp(-gamma * (D^2 / outer(l^2, rep(1, m), "*")))
  
  return(K)
}

```


## Adding Noise to Kernels

## Combining Kernels

Kernels can be combined in several ways to create more powerful and flexible kernel functions. Here are some common ways to combine kernels:

### Combined Homogenous Kernels

Summation: Two or more kernels can be added together to create a new kernel that captures the features of both. For example, if you have two kernels K1 and K2, you can combine them as K = K1 + K2. The resulting kernel K will assign high values to pairs of points that are similar in either K1 or K2.

Product: Two or more kernels can be multiplied together to create a new kernel that captures the features of both. For example, if you have two kernels K1 and K2, you can combine them as K = K1 * K2. The resulting kernel K will assign high values to pairs of points that are similar in both K1 and K2.

Convolution: Two or more kernels can be convolved together to create a new kernel that captures the features of both. For example, if you have two kernels K1 and K2, you can combine them as K = K1 * K2, where * represents convolution. The resulting kernel K will assign high values to pairs of points that are similar in both K1 and K2, but will also capture more complex patterns that cannot be captured by either kernel alone.

Weighted combination: Two or more kernels can be combined as a weighted sum to create a new kernel that assigns different weights to each kernel. For example, if you have two kernels K1 and K2, you can combine them as K = w1 * K1 + w2 * K2, where w1 and w2 are the weights assigned to K1 and K2, respectively. The resulting kernel K will assign high values to pairs of points that are similar in either K1 or K2, but the weights can be adjusted to give more importance to one kernel over the other.

Power: A kernel can be raised to a power to create a new kernel that captures higher-order patterns. For example, if you have a kernel K, you can raise it to the power of p as K = K^p. The resulting kernel K will assign high values to pairs of points that are similar in K, but will also capture higher-order patterns that cannot be captured by K alone.

By combining kernels in different ways, you can create more complex and flexible kernel functions that can capture a wide range of patterns in the input data.

### Combined Heterogenous Kernels

The idea behind using heterogeneous kernels is to allow the model to capture different types of patterns in different dimensions of the input data. For example, if you have a dataset with features that have different types of scales, you might want to use a linear kernel for the features that are on a similar scale and a radial basis function (RBF) kernel for the features that have a different scale. Another example is in natural language processing, where a bag-of-words representation can be combined with a kernel that captures syntactic or semantic relations between words.

```{r}
mixed_kernel <- function(X1, X2, gamma, l, kernel_type) {
  # X1: n x p matrix
  # X2: m x p matrix
  # gamma: scalar, the kernel width parameter
  # l: p-dimensional vector, the length scale parameter
  # kernel_type: p-dimensional vector, specifies the kernel type for each dimension
  
  n <- nrow(X1)
  m <- nrow(X2)
  p <- ncol(X1)
  
  # Initialize kernel matrix to zeros
  K <- matrix(0, n, m)
  
  for (i in 1:p) {
    # Compute kernel matrix for current dimension
    if (kernel_type[i] == "rbf") {
      K_i <- exp(-gamma * ((X1[,i] %*% t(rep(1,m)))^2 + (rep(1,n) %*% t(X2[,i]))^2 - 2*X1[,i] %*% t(X2[,i])) / l[i]^2)
    } else if (kernel_type[i] == "linear") {
      K_i <- X1[,i] %*% t(X2[,i])
    } else {
      stop("Unsupported kernel type")
    }
    
    # Add kernel matrix to mixed kernel matrix
    K <- K + K_i
  }
  
  return(K)
}

```

# References {-}

<div id="refs"></div>

# License

The content of this project itself is licensed under the [Creative Commons Attribution-ShareAlike 4.0 International license](https://creativecommons.org/licenses/by-sa/4.0/), and the underlying code is licensed under the [GNU General Public License v3.0 license](https://github.com/AnHosu/bespoke-bayesian-biochem/blob/main/LICENSE).
