---
title: ""
csl: ../citation_style.csl
format:
  html:
    fig-width: 8
    fig-height: 5
    theme: none
html-math-method:
  method: katex
---

Bayesian optimization is a powerful technique for optimizing black-box functions, particularly when the number of evaluations of the objective function is limited. It has found applications in various fields, including hyperparameter tuning, model selection, and optimization of physical experiments. In this blog post, we will discuss the role of the objective function in Bayesian optimization and provide a practical guide on how to benchmark Bayesian optimization algorithms using synthetic test functions. Our aim is to equip engineers with the knowledge and tools required to apply Bayesian optimization to real-world problems effectively.
```{r setup, echo=TRUE}
library(ggplot2)
library(magrittr)
seed <- 4444
set.seed(seed)
```


```{r}
ackley <- function(X) {
  if (is.null(dim(X))) dim(X) <- c(1, length(X))
  d <- ncol(X)
  part1 <- -20*exp(-0.2*sqrt(1/d*rowSums(X^2)))
  part2 <- -exp(1/d*rowSums(cos(2*pi*X)))
  part1 + part2 + 20 + exp(1)
}

lspace <- seq(-1.5, 1.5, 0.01)
obs <- lspace %>%
  tidyr::expand_grid(x1 = ., x2 = .) %>%
  dplyr::mutate(y = ackley(cbind(x1, x2)))
  
plotly::plot_ly(
  x = lspace,
  y = lspace,
  z = `dim<-`(obs$y, c(length(lspace), length(lspace))),
  showscale = FALSE
) %>%
  plotly::add_surface(
    contours = list(

    z = list(

      show=TRUE,
      start = 0.5, end = 7, size = 0.5, color = "white",
      usecolormap=TRUE,

      highlightcolor="#ff0000",

      project=list(z=TRUE)

      )

    )
  ) %>%
  plotly::layout(
    plot_bgcolor = "rgba(0,0,0,0)",
    paper_bgcolor = "rgba(0,0,0,0)",
    scene = list(

      camera=list(

        eye = list(x=2.2, y=1.8, z=1.5)

        )

      )

  )
```


## The Role of the Objective Function in Bayesian Optimization

In Bayesian optimization, the objective function is the black-box function that we aim to optimize. It maps input parameters to a scalar output, which we want to minimize or maximize. The objective function is usually expensive to evaluate, noisy, and lacks an analytical expression. Bayesian optimization aims to find the global optimum of the objective function with the fewest evaluations possible.

The objective function, denoted by $f(\mathbf{x})$, is a mapping from the input space $\mathbf{x} \in \mathcal{X}$ to a scalar output $y \in \mathbb{R}$:

$$f: \mathcal{X} \rightarrow \mathbb{R}$$

In Bayesian optimization, we model the objective function using a surrogate model, typically a Gaussian process, which is cheap to evaluate and provides uncertainty estimates. The algorithm then uses an acquisition function to balance exploration and exploitation and decides on the next point to evaluate in the search space.

## How to Benchmark Bayesian Optimization

Benchmarking Bayesian optimization is essential for understanding its performance, ensuring its reliability, and comparing different algorithms. To benchmark Bayesian optimization, we can use synthetic test functions that exhibit various properties found in real-world problems. These properties include multi-modality, non-convexity, varying levels of noise, and a mix of continuous and discrete features.

A comprehensive benchmark should include:

    Multiple test functions with varying properties.
    A diverse set of initial points to account for the effect of initial conditions.
    A fixed budget of evaluations to measure the efficiency of the optimization algorithm.
    A comparison of the optimization results against the known global optimum.

By comparing the performance of the Bayesian optimization algorithm on these test functions, we can gain insights into its strengths and weaknesses and determine its applicability to real-world problems.

To benchmark Bayesian optimization using the given Gaussian process gp for each of the three test functions, we will follow these steps:

    Define a search space for each test function, including continuous, discrete, and categorical variables as necessary.
    Implement an acquisition function, such as Expected Improvement (EI), to balance exploration and exploitation.
    Run the Bayesian optimization algorithm for a fixed budget of evaluations.
    Compare the optimization results against the known global optima.



## Examples of Test Functions
#### Branin-Hoo Function

The Branin-Hoo function is a widely used test function in global optimization. It is a continuous, 2-dimensional function with three global minima and a smooth landscape.

$$f(\mathbf{x}) = (x_2 - \frac{5.1}{4\pi^2}x_1^2 + \frac{5}{\pi}x_1 - 6)^2 + 10(1 - \frac{1}{8\pi})\cos(x_1) + 10$$

where $\mathbf{x} = [x_1, x_2] \in [-5, 10] \times [0, 15]$.

```{r}
# Branin-Hoo Function
branin_hoo <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  
  term1 <- (x2 - (5.1 / (4 * pi^2)) * x1^2 + (5 / pi) * x1 - 6)^2
  term2 <- 10 * (1 - (1 / (8 * pi))) * cos(x1) + 10
  
  return(term1 + term2)
}
```

#### Mixed-Integer Ackley Function

The Ackley function is another popular test function for optimization, and it can be modified to include both continuous and discrete features. The mixed-integer Ackley function is defined as:

$$f(\mathbf{x}) = -20 \exp\left(-0.2\sqrt{\frac{1}{n}\sum_{i=1}^{n} x_i^2}\right) - \exp\left(\frac{1}{n}\sum_{i=1}^{n} \cos(2\pi x_i)\right) + 20 + e$$

where $\mathbf{x} = [x_1, x_2, ..., x_n] \in \mathcal{X}$, and $\mathcal{X}$ is the mixed-integer search space defined as:

$$\mathcal{X} = \{x_i \in \mathbb{Z} \cap [-5, 5], i = 1, 2, ..., m\} \times \{x_i \in \mathbb{R} \cap [-5, 5], i = m + 1, m + 2, ..., n\}$$

In this definition, the first $m$ dimensions are discrete, and the remaining $n - m$ dimensions are continuous. The global minimum of the Mixed-Integer Ackley function is $f(\mathbf{x^}) = 0$ at $\mathbf{x^} = [0, 0, ..., 0]$.

```{r}
# Mixed-Integer Ackley Function
mixed_integer_ackley <- function(x, m) {
  n <- length(x)
  sum_sq <- sum(x^2)
  sum_cos <- sum(cos(2 * pi * x))
  
  term1 <- -20 * exp(-0.2 * sqrt(sum_sq / n))
  term2 <- -exp(sum_cos / n)
  term3 <- 20 + exp(1)
  
  return(term1 + term2 + term3)
}
```

#### One-Hot Encoded Categorical Test Function

In real-world applications, we often encounter optimization problems with categorical features. To test Bayesian optimization algorithms on problems with categorical variables, we can create a synthetic test function with one-hot encoded categorical features. One-hot encoding is a common technique used to represent categorical data as binary vectors.

Let's consider a mixed-variable optimization problem with continuous, discrete, and categorical features. The search space is defined as:

$$\mathcal{X} = \{x_i \in \mathbb{Z} \cap [0, 5], i = 1, 2, ..., m\} \times \{x_i \in \mathbb{R} \cap [-5, 5], i = m + 1, m + 2, ..., n\} \times \{x_i \in \mathcal{C}, i = n + 1, n + 2, ..., n + p\}$$

Here, $\mathcal{C}$ represents the one-hot encoded categorical features.

For simplicity, let's create a test function that combines the Branin-Hoo function with an additional categorical variable:

$$f(\mathbf{x}) = \text{Branin-Hoo}(x_1, x_2) + w(\mathbf{c})$$

where $w(\mathbf{c})$ is a weight function that maps the categorical variable $\mathbf{c}$ to a scalar value:

$$w(\mathbf{c}) = \begin{cases}
10 &amp; \text{if } \mathbf{c} = [1, 0, 0] \\
20 &amp; \text{if } \mathbf{c} = [0, 1, 0] \\
30 &amp; \text{if } \mathbf{c} = [0, 0, 1]
\end{cases}$$

```{r}
# # Weight function for the categorical variable
# weight_function <- function(c) {
#   if (all(c == c(1, 0, 0))) {
#     return(10)
#   } else if (all(c == c(0, 1, 0))) {
#     return(20)
#   } else if (all(c == c(0, 0, 1))) {
#     return(30)
#   } else {
#     stop("Invalid categorical input")
#   }
# }
# 
# # Combined Branin-Hoo function with categorical features
# branin_hoo_categorical <- function(x, c) {
#   return(branin_hoo(x) + weight_function(c))
# }
```


```{r}
# bayesian_optimization <- function(gp, acquisition_function, X_initial, y_initial, max_iter, test_function, ...) {
#   # Start with initial observations
#   X <- X_initial
#   y <- y_initial
#   
#   for (i in 1:max_iter) {
#     # Fit the Gaussian process model using the current observations
#     gp.fit(X, y)
#     
#     # Use the acquisition function to select the next point to evaluate
#     x_next <- acquisition_function(gp, ...)
#     
#     # Evaluate the test function at the chosen point
#     y_next <- test_function(x_next, ...)
#     
#     # Update the observations
#     X <- rbind(X, x_next)
#     y <- c(y, y_next)
#   }
#   
#   # Return the best solution found
#   return(list("x_best" = X[which.min(y),], "y_best" = min(y)))
# }
# 
# expected_improvement <- function(gp, X_pred, y_min, xi = 0.01) {
#   mu <- gp.predict_mean(X_pred)
#   sigma <- gp.predict_sd(X_pred)
#   
#   z <- (mu - y_min - xi) / sigma
#   ei <- (mu - y_min - xi) * pnorm(z) + sigma * dnorm(z)
#   return(ei)
# }
# 
# 
# # Benchmarking for Branin-Hoo Function
# X_initial_bh <- ... # Initial points for the Branin-Hoo function
# y_initial_bh <- apply(X_initial_bh, 1, branin_hoo)
# 
# result_bh <- bayesian_optimization(gp, expected_improvement, X_initial_bh, y_initial_bh, max_iter = 50, test_function = branin_hoo)
# 
# # Benchmarking for Mixed-Integer Ackley Function
# X_initial_mia <- ... # Initial points for the Mixed-Integer Ackley function
# y_initial_mia <- apply(X_initial_mia, 1, mixed_integer_ackley, m = ...)
# 
# result_mia <- bayesian_optimization(gp, expected_improvement, X_initial_mia, y_initial_mia, max_iter = 50, test_function = mixed_integer_ackley, m = ...)
# 
# # Benchmarking for Branin-Hoo Function with Categorical Features
# X_initial_bhc <- ... # Initial points for the Branin-Hoo function with categorical features
# y_initial_bhc <- apply(X_initial_bhc[, 1:2], 1, branin_hoo_categorical, c = X_initial_bhc[, 3:5])
# 
# result_bhc <- bayesian_optimization(gp, expected_improvement, X_initial_bhc, y_initial_bhc, max_iter = 50, test_function = branin_hoo_categorical)

```

