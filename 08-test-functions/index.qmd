---
title: ""
bibliography: references.bib
csl: ../citation_style.csl
format:
  html:
    fig-width: 8
    fig-height: 5
    theme: none
html-math-method:
  method: katex
---

[Bayesian optimisation](../bayes-opt-r) is a powerful optimisation technique for black-box functions and processes with expensive evaluations. It is popular for hyperparameter tuning in machine learning, but has many real-world applications as well.

At the centre of Bayesian optimisation is the objective function that we are trying to optimise. The objective function is usually expensive to evaluate, so before we start using Bayesian optimisation on the actual problem, we want to make sure that our code and models are working. In this post, we discuss a set of functions that can help us test and gauge the efficacy of our models. Along with the discussions are implementations in R.

```{r setup, echo=TRUE}
library(ggplot2)
library(magrittr)
set.seed(4444)
set_dim <- `dim<-`
```

## Objective Functions in Bayesian Optimisation

The objective function or process, $f$, 

The objective function, denoted by $f(\mathbf{x})$, is a mapping from the input space $\mathbf{x} \in \mathcal{X}$ to a scalar output $y \in \mathbb{R}$:

$$f: \mathcal{X} \rightarrow \mathbb{R}$$

In Bayesian optimization, we model the objective function using a surrogate model, typically a Gaussian process, which is cheap to evaluate and provides uncertainty estimates. The algorithm then uses an acquisition function to balance exploration and exploitation and decides on the next point to evaluate in the search space.

#### An Example Model

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#' RBF Kernel
#'
#' @param X1 matrix of dimensions (n, d). Vectors are coerced to (1, d).
#' @param X2 matrix of dimensions (m, d). Vectors are coerced to (1, d).
#' @param l length scale
#' @param sigma_f scale parameter 
#'
#' @return matrix of dimensions (n, m)
rbf_kernel <- function(X1, X2, l = 1.0, sigma_f = 1.0) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  sqdist <- (- 2*(X1 %*% t(X2))) %>%
    add(rowSums(X1**2, dims = 1)) %>%
    sweep(2, rowSums(X2**2, dims = 1), `+`)
  sigma_f**2 * exp(-0.5 / l**2 * sqdist)
}

#' Get Parameters of the Posterior Gaussian Process
#'
#' @param kernel kernel function used for the Gaussian process
#' @param X_pred matrix (m, d) of prediction points
#' @param X_train matrix (n, d) of training points
#' @param y_train column vector (n, d) of training observations
#' @param noise scalar of observation noise
#' @param ... named parameters for the kernel function
#'
#' @return list of mean (mu) and covariance (sigma) for the Gaussian
posterior <- function(kernel, X_pred, X_train, y_train, noise = 1e-8, ...) {
  if (is.null(dim(X_pred))) dim(X_pred) <- c(length(X_pred), 1)
  if (is.null(dim(X_train))) dim(X_train) <- c(length(X_train), 1)
  if (is.null(dim(y_train))) dim(y_train) <- c(length(y_train), 1)
  K <- kernel(X_train, X_train, ...) + noise**2 * diag(dim(X_train)[[1]])
  K_s <- kernel(X_train, X_pred, ...)
  K_ss <- kernel(X_pred, X_pred, ...) + 1e-8 * diag(dim(X_pred)[[1]])
  K_inv <- solve(K)
  mu <- (t(K_s) %*% K_inv) %*% y_train
  sigma <- K_ss - (t(K_s) %*% K_inv) %*% K_s
  list(mu = mu, sigma = sigma)
}

#' Gaussian Negative log-Likelihood of a Kernel
#'
#' @param kernel kernel function
#' @param X_train matrix (n, d) of training points
#' @param y_train column vector (n, d) of training observations
#' @param noise scalar of observation noise
#'
#' @return function with kernel parameters as input and negative log likelihood
#' as output
nll <- function(kernel, X_train, y_train, noise) {
  function(params) {
    n <- dim(X_train)[[1]]
    K <- rlang::exec(kernel, X1 = X_train, X2 = X_train, !!!params)
    L <- chol(K + noise**2 * diag(n))
    a <- backsolve(r = L, x = forwardsolve(l = t(L), x = y_train))
    0.5*t(y_train)%*%a + sum(log(diag(L))) + 0.5*n*log(2*pi)
  }
}

#' Gaussian Process Regression
#'
#' @param kernel kernel function
#' @param X_train matrix (n, d) of training points
#' @param y_train column vector (n, d) of training observations
#' @param noise scalar of observation noise
#' @param ... parameters of the kernel function with initial guesses. Due to the
#' optimiser used, all parameters must be given and the order unfortunately
#' matters
#'
#' @return function that takes a matrix of prediction points as input and
#' returns the posterior predictive distribution for the output
gpr <- function(kernel, X_train, y_train, noise = 1e-8, ...) {
  kernel_nll <- nll(kernel, X_train, y_train, noise)
  param <- list(...)
  opt <- optim(par = rep(1, length(param)), fn = kernel_nll)
  opt_param <- opt$par
  function(X_pred) {
    post <- rlang::exec(
      posterior,
      kernel = kernel,
      X_pred = X_pred,
      X_train = X_train,
      y_train = y_train,
      noise = noise,
      !!!opt_param
    )
    list(
      mu = post$mu,
      sigma = diag(post$sigma),
      Sigma = post$sigma,
      parameters = set_names(opt_param, names(param))
    )
  }
}

#' Expected Improvement Acquisition Function for a Gaussian Surrogate
#' 
#' @param mu vector of length m. Mean of a Gaussian process at m points.
#' @param sigma vector of length m. The diagonal of the covariance matrix of a
#' Gaussian process evaluated at m points.
#' @param y_best scalar. Best mean prediction so far on observed points
#' @param xi scalar, exploration/exploitation trade off
#' @param task one of "max" or "min", indicating the optimisation problem
#'
#' @return EI, vector of length m
expected_improvement <- function(mu, sigma, y_best, xi = 0.01, task = "min") {
  if (task == "min") imp <- y_best - mu - xi
  if (task == "max") imp <- mu - y_best - xi
  if (is.null(imp)) stop('task must be "min" or "max"')
  Z <- imp / sigma
  ei <- imp * pnorm(Z) + sigma * dnorm(Z)
  ei[sigma == 0.0] <- 0.0
  ei
}

#' Plot of a Gaussian Process in One Dimension
#' 
#' @param mu vector of length m. Mean of a Gaussian process at m points.
#' @param sigma vector of length m. The diagonal of the covariance matrix of a
#' Gaussian process evaluated at m points.
#' @param X_pred matrix of dimensions (m X 1) representing m prediction points 
#' with one dimension.
#' @param X_train matrix of dimensions (n X 1) representing n training points
#' with one dimension
#' @param y_train vector of length n representing n observations at points
#' X_train
#' @param true_function function representing the objective function (in real
#' life, this function is unknown and cannot be plotted)
#'
#' @return ggplot2 plot
gp_1d_plot <- function(mu, sigma, X_pred, X_train, y_train, true_function) {
  tibble::tibble(
    m = mu,
    uncertainty = 1.96*sqrt(sigma),
    upper = m + uncertainty,
    lower = m - uncertainty,
    x = X_pred,
    f = true_function(X_pred)
  ) %>%
    ggplot(aes(x = x)) +
    geom_line(aes(y = m, colour = "Mean")) +
    geom_ribbon(
      aes(ymin = lower, ymax = upper, fill = "89% interval"),
      alpha = 0.2
    ) +
    geom_point(
      data = tibble::tibble(x = X_train, y = y_train),
      aes(x = x, y = y, shape = "Training point"),
      colour = "#fb8500",
      size = 4
    ) +
    geom_line(mapping = aes(y = f, colour = "True function")) +
    scale_shape_manual(values = c("Training point" = "+")) +
    scale_fill_manual(values = c("89% interval" = "#219ebc")) +
    labs(shape = "") +
    theme_minimal() +
    labs(
      y = "y",
      x = "",
      colour = "",
      fill = ""
    ) +
    theme(panel.grid = element_blank(), axis.text.x = element_blank())
}

#' Plot of Acquisition Function with Surrogate in One Dimension
#' 
#' @X_pred matrix of dimensions (m X 1) representing m prediction points with 
#' one dimension.
#' @acquisition_function vector of length m representing the acquisition
#' function evaluated at the m points of X_pred
#' @param uncertainty_plot the plot of a surrogate model in one dimension
#' @param xt1 scalar, the point, x, that optimises the acquisition function
#' @param label character, label for the acquisition function
#' @param title character, a title for the plot
#'
#' @return ggplot2 plot
acquisition_plot <- function(X_pred,
                             acquisition_function,
                             uncertainty_plot,
                             xt1,
                             label = "EI",
                             title = "") {
  p1 <- tibble::tibble(
    x = X_pred,
    a = acquisition_function
  ) %>%
    ggplot() +
    geom_line(aes(x = x, y = a, colour = label)) +
    geom_vline(xintercept = xt1, linetype = 2) +
    theme_minimal() +
    labs(x = "", y = label, colour = "") +
    theme(panel.grid = element_blank())
  p2 <- uncertainty_plot +
    geom_vline(xintercept = xt1, linetype = 2) +
    labs(title = title)
  aligned_plots <- cowplot::align_plots(p2, p1 , align = "v")
  cowplot::plot_grid(aligned_plots[[1]], aligned_plots[[2]], ncol = 1)
}

gp_rbf_ei_1d <- function(X_train, X_pred, demo_function, noise = 1e-8, title = "" ) {
  y_train <- demo_function(X_train) + rnorm(nrow(X_train), 0, noise)
  gp <- gpr(
    kernel = rbf_kernel,
    X_train = X_train,
    y_train = y_train,
    noise = noise,
    l = 1,
    sigma_f = 1
  )
  post_pred <- gp(X_pred)
  mu <- post_pred$mu
  sigma <- post_pred$sigma
  ei <- expected_improvement(mu = mu, sigma = sigma, y_best = min(y_train))
  gp_plot <- gp_1d_plot(
    mu = mu,
    sigma = sigma,
    X_pred = X_pred,
    X_train = X_train,
    y_train = y_train,
    true_function = demo_function
  )
  acquisition_plot(
    X_pred = X_pred,
    acquisition_function = ei,
    uncertainty_plot = gp_plot,
    xt1 = X_pred[which.max(ei)],
    label = "EI",
    title = paste("Gaussian Process Surrogate", title)
  )
}
```


```{r}
demo_objective_function <- function(x) sin(12 * x) * x + 0.5 * x^2
X_train <-  matrix(c(0.02, 0.3, 0.55, 0.75, 0.8, 0.98), 6, 1)
noise <- 0.05
y_train <- demo_objective_function(X_train) + rnorm(6, 0, noise)
X_pred <- matrix(seq(0, 1, length.out = 100), 100, 1)
gp_rbf_ei_1d(
  X_train = X_train,
  X_pred = X_pred,
  demo_function = demo_objective_function,
  noise = noise
)
```

## Test Functions in n Continuous Dimensions

## Ackley

There are several Ackley test functions, one of them is defined as [@Jamil2013]:

$$f(\mathbf{x}) = -a\exp\left(-b\sqrt{\frac{1}{d}\sum_{i=1}^dx_i^2}\right) - \exp\left(\frac{1}{d}\sum_{i=1}^d \cos(c x_i)\right) \\ + a + \exp(1)$$

where $d$ is the number of dimensions and $a$, $b$, and $c$ are constants, which are set at $a = 20$, $b = 0.2$, and $c = 2\pi$.

```{r}
ackley <- function(X) {
  if (is.null(dim(X))) set_dim(X, c(1, length(X)))
  d <- ncol(X)
  part1 <- -20 * exp(-0.2 * sqrt(1 / d * rowSums(X^2)))
  part2 <- -exp(1 / d * rowSums(cos(2 * pi * X)))
  part1 + part2 + 20 + exp(1)
}
```

$\mathbf{x}$ is limited to $-35 > x_i < 35$, though the range $-5 < x_i < 5$ is plenty challenging. The global minimum of the Ackley function is $f(\mathbf{x})=0$ at $\mathbf{x}=(0,0,...,0)$.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
ackley_scale <- function(X) X * 10 - 5

n <- 300
x <- seq(-2, 2, length.out = n)
y <- expand.grid(x , x) %>%
  ackley() %>%
  set_dim(c(n, n))

plotly::plot_ly(
  x = x,
  y = x,
  z = y,
  showscale = FALSE
) %>%
  plotly::add_surface(
    contours = list(
      z = list(
        show = TRUE,
        start = 0.5,
        end = 7,
        size = 0.5,
        color = "white",
        usecolormap = TRUE,
        highlightcolor = "#ff0000",
        project = list(z = TRUE)
      )
    )
  ) %>%
  plotly::layout(
    plot_bgcolor = "rgba(0,0,0,0)",
    paper_bgcolor = "rgba(0,0,0,0)",
    scene = list(camera = list(eye = list(x = 2.2, y = 1.8, z = 1.5)))
  )
```

When conditioning a GP with an RBF kernel on the 1D Ackley function, the global trend is fairly easily captured. The periodic component is not captured. The use of a periodic kernel along with the RBF kernel would possibly remedy that, but it is not necessary when we are joust looking for the global minimum.

```{r}
gp_rbf_ei_1d(
  X_train = ackley_scale(X_train),
  X_pred = ackley_scale(X_pred),
  demo_function = ackley,
  noise = 0.4,
  title = "on the 1D Ackley Function"
)
```


#### Branin-Hoo Function

The Branin-Hoo function is a widely used test function in global optimization. It is a continuous, 2-dimensional function with three global minima and a smooth landscape.

$$f(\mathbf{x}) = (x_2 - \frac{5.1}{4\pi^2}x_1^2 + \frac{5}{\pi}x_1 - 6)^2 + 10(1 - \frac{1}{8\pi})\cos(x_1) + 10$$

where $\mathbf{x} = [x_1, x_2] \in [-5, 10] \times [0, 15]$.

```{r}
# Branin-Hoo Function
branin_hoo <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  
  term1 <- (x2 - (5.1 / (4 * pi^2)) * x1^2 + (5 / pi) * x1 - 6)^2
  term2 <- 10 * (1 - (1 / (8 * pi))) * cos(x1) + 10
  
  return(term1 + term2)
}
```






## Michalewicz




#### Mixed-Integer Ackley Function

The Ackley function is another popular test function for optimization, and it can be modified to include both continuous and discrete features. The mixed-integer Ackley function is defined as:

$$f(\mathbf{x}) = -20 \exp\left(-0.2\sqrt{\frac{1}{n}\sum_{i=1}^{n} x_i^2}\right) - \exp\left(\frac{1}{n}\sum_{i=1}^{n} \cos(2\pi x_i)\right) + 20 + e$$

where $\mathbf{x} = [x_1, x_2, ..., x_n] \in \mathcal{X}$, and $\mathcal{X}$ is the mixed-integer search space defined as:

$$\mathcal{X} = \{x_i \in \mathbb{Z} \cap [-5, 5], i = 1, 2, ..., m\} \times \{x_i \in \mathbb{R} \cap [-5, 5], i = m + 1, m + 2, ..., n\}$$

In this definition, the first $m$ dimensions are discrete, and the remaining $n - m$ dimensions are continuous. The global minimum of the Mixed-Integer Ackley function is $f(\mathbf{x^}) = 0$ at $\mathbf{x^} = [0, 0, ..., 0]$.

```{r}
# Mixed-Integer Ackley Function
mixed_integer_ackley <- function(x, m) {
  n <- length(x)
  sum_sq <- sum(x^2)
  sum_cos <- sum(cos(2 * pi * x))
  
  term1 <- -20 * exp(-0.2 * sqrt(sum_sq / n))
  term2 <- -exp(sum_cos / n)
  term3 <- 20 + exp(1)
  
  return(term1 + term2 + term3)
}
```

#### One-Hot Encoded Categorical Test Function

In real-world applications, we often encounter optimization problems with categorical features. To test Bayesian optimization algorithms on problems with categorical variables, we can create a synthetic test function with one-hot encoded categorical features. One-hot encoding is a common technique used to represent categorical data as binary vectors.

Let's consider a mixed-variable optimization problem with continuous, discrete, and categorical features. The search space is defined as:

$$\mathcal{X} = \{x_i \in \mathbb{Z} \cap [0, 5], i = 1, 2, ..., m\} \times \{x_i \in \mathbb{R} \cap [-5, 5], i = m + 1, m + 2, ..., n\} \times \{x_i \in \mathcal{C}, i = n + 1, n + 2, ..., n + p\}$$

Here, $\mathcal{C}$ represents the one-hot encoded categorical features.

For simplicity, let's create a test function that combines the Branin-Hoo function with an additional categorical variable:

$$f(\mathbf{x}) = \text{Branin-Hoo}(x_1, x_2) + w(\mathbf{c})$$

where $w(\mathbf{c})$ is a weight function that maps the categorical variable $\mathbf{c}$ to a scalar value:

$$w(\mathbf{c}) = \begin{cases}
10 &amp; \text{if } \mathbf{c} = [1, 0, 0] \\
20 &amp; \text{if } \mathbf{c} = [0, 1, 0] \\
30 &amp; \text{if } \mathbf{c} = [0, 0, 1]
\end{cases}$$

```{r}
# # Weight function for the categorical variable
# weight_function <- function(c) {
#   if (all(c == c(1, 0, 0))) {
#     return(10)
#   } else if (all(c == c(0, 1, 0))) {
#     return(20)
#   } else if (all(c == c(0, 0, 1))) {
#     return(30)
#   } else {
#     stop("Invalid categorical input")
#   }
# }
# 
# # Combined Branin-Hoo function with categorical features
# branin_hoo_categorical <- function(x, c) {
#   return(branin_hoo(x) + weight_function(c))
# }
```

# References {-}

<div id="refs"></div>

# License

The content of this project itself is licensed under the [Creative Commons Attribution-ShareAlike 4.0 International license](https://creativecommons.org/licenses/by-sa/4.0/), and the underlying code is licensed under the [GNU General Public License v3.0 license](https://github.com/AnHosu/bayes-opt/blob/6e25a7a4ec88edac9b55dea2b51382d21030a998/LICENSE).
