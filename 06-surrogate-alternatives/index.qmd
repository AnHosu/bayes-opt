---
title: ""
csl: ../citation_style.csl
format:
  html:
    fig-width: 8
    fig-height: 5
    theme: default
html-math-method:
  method: katex
---

[Bayesian optimisation](../bayes-opt-r) is a powerful optimisation technique for black-box functions and processes with expensive evaluations. It is popular for hyperparameter tuning and model selection in machine learning, but has many real-world applications as well. One of the key components of Bayesian optimisation is the surrogate model, which models the objective function and helps guide optimisation by being a cheap to evaluate representation of our posterior beliefs. Gaussian processes are commonly used as surrogate functions, as they offer many of the qualities we need when doing Bayesian optimisation out of the box. However, there are alternatives to Gaussian processes and, in this post, we will dive into the role of surrogate models in Bayesian optimisation, focusing on models that are not Gaussian processes.

Along with the discussion, are implementations in R.

```{r func, echo=TRUE}
library(ggplot2)
library(magrittr)
seed <- 4444
set.seed(seed)
```

## Surrogate Models in Bayesian Optimisation

In Bayesian optimisation, we are conducting experiments on an objective function or process, $f$, that is very expensive to evaluate. The objective function is evaluated on a search space $\mathcal{X}$, and we are looking for the point, $\mathbf{x} \in \mathcal{X}$, that optimises the objective function. We do this in a sequential manner, where we consider the next point, $\mathbf{x}_{t+1}$, to evaluate after sampling the previous point. To help decide on the next point, we employ an acquisition function, $a(\mathbf{x})$, that should be easy to optimise on the search space:

$$\mathbf{x}_{t+1} = \arg\min_{\mathbf{x} \in \mathcal{X}} a(\mathbf{x})$$

The acquisition function is itself a function of the surrogate model. The surrogate is a regression model that is used to approximate the objective function and it provides predictions of the objective function values and uncertainty estimates at unobserved points in the input space.

Specifically, many acquisition functions are functions of the mean $\mu(\mathbf{x})$ and standard deviation $\sigma(\mathbf{x})$ across the search space. In those cases, the surrogate model should provide estimates of those two.

Gaussian processes, are often used as surrogate models because it is easy to isolate those two components. $\mu(\mathbf{x})$ is simply the posterior mean function and $\sigma(\mathbf{x})$ is an entry on the diagonal of the posterior covariance matrix.

There are, however, alternatives to Gaussian processes as surrogates and some of them are discussed below. While there are [acquisition functions](.../post/acquisition-functions-r/) that rely on more complex mechanisms, this post focuses on surrogate models that support acquisition functions that are in some way a function of $\mu(\mathbf{x})$ and $\sigma(\mathbf{x})$.

#### An Example Problem

For the demonstration of acquisition functions, we need a toy problem. We will use a simple objective function without noise and a single dimension.

```{r}
objective_function <- function(x) {
  sin(12 * x) * x + 0.5 * x^2
}
```

This function has two minima and two maxima in the search space $\mathcal{X} = [0,1]$, so it will not be too easy to maximise or minimise.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
ggplot() +
  geom_function(fun = objective_function) +
  xlim(c(0, 1)) +
  theme_minimal() +
  labs(x = "x", y = "f(x)", title = "Objective Function")
```

We will approximate the the objective function with different surrogate models.

The surrogate model will receive four training points and it should be able to return $\mu(\mathbf{x})$ and $\sigma(\mathbf{x})$ for all points in the search space, which we will approximate with a grid.

```{r}
X_train <-  matrix(c(0.02, 0.3, 0.5, 0.75, 0.98), 5, 1) # matrix(seq(0, 1, 0.1))
y_train <- objective_function(X_train) + rnorm(5, 0, 0.05)
X_pred <- matrix(seq(0, 1, length.out = 100), 100, 1)
```

For reference, here is a Gaussian process conditioned on the four training points

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#' RBF Kernel
#'
#' @param X1 matrix of dimensions (n, d). Vectors are coerced to (1, d).
#' @param X2 matrix of dimensions (m, d). Vectors are coerced to (1, d).
#' @param l length scale
#' @param sigma_f scale parameter 
#'
#' @return matrix of dimensions (n, m)
rbf_kernel <- function(X1, X2, l = 1.0, sigma_f = 1.0) {
  if (is.null(dim(X1))) dim(X1) <- c(1, length(X1))
  if (is.null(dim(X2))) dim(X2) <- c(1, length(X2))
  sqdist <- (- 2*(X1 %*% t(X2))) %>%
    add(rowSums(X1**2, dims = 1)) %>%
    sweep(2, rowSums(X2**2, dims = 1), `+`)
  sigma_f**2 * exp(-0.5 / l**2 * sqdist)
}

#' Random Samples from a Multivariate Gaussian
#' 
#' This implementation is similar to MASS::mvrnorm, but uses chlosky
#' decomposition instead. This should be more stable but is less efficient than
#' the MASS implementation, which recycles the eigen decomposition for the
#' sampling part.
#'
#' @param n number of samples to sample
#' @param mu the mean of each input dimension
#' @param sigma the covariance matrix
#' @param epsilon numerical tolerance added to the diagonal of the covariance
#'  matrix. This is necessary for the Cholesky decomposition, in some cases.
#'
#' @return numerical vector of n samples
rmvnorm <- function(n = 1, mu, sigma, epsilon = 1e-6) {
    p <- length(mu)
    if(!all(dim(sigma) == c(p, p))) stop("incompatible dimensions of arguments")
    ev <- eigen(sigma, symmetric = TRUE)$values
    if(!all(ev >= -epsilon*abs(ev[1L]))) {
      stop("The covariance matrix (sigma) is not positive definite")
    }
    cholesky <- chol(sigma + diag(p)*epsilon)
    sample <- rnorm(p*n, 0, 1)
    dim(sample) <- c(n, p)
    sweep(sample %*% cholesky, 2, mu, FUN = `+`)
}

#' Get Parameters of the Posterior Gaussian Process
#'
#' @param kernel kernel function used for the Gaussian process
#' @param X_pred matrix (m, d) of prediction points
#' @param X_train matrix (n, d) of training points
#' @param y_train column vector (n, d) of training observations
#' @param noise scalar of observation noise
#' @param ... named parameters for the kernel function
#'
#' @return list of mean (mu) and covariance (sigma) for the Gaussian
posterior <- function(kernel, X_pred, X_train, y_train, noise = 1e-8, ...) {
  if (is.null(dim(X_pred))) dim(X_pred) <- c(length(X_pred), 1)
  if (is.null(dim(X_train))) dim(X_train) <- c(length(X_train), 1)
  if (is.null(dim(y_train))) dim(y_train) <- c(length(y_train), 1)
  K <- kernel(X_train, X_train, ...) + noise**2 * diag(dim(X_train)[[1]])
  K_s <- kernel(X_train, X_pred, ...)
  K_ss <- kernel(X_pred, X_pred, ...) + 1e-8 * diag(dim(X_pred)[[1]])
  K_inv <- solve(K)
  mu <- (t(K_s) %*% K_inv) %*% y_train
  sigma <- K_ss - (t(K_s) %*% K_inv) %*% K_s
  list(mu = mu, sigma = sigma)
}

#' Negative log-Likelihood of a Kernel
#'
#' @param kernel kernel function
#' @param X_train matrix (n, d) of training points
#' @param y_train column vector (n, d) of training observations
#' @param noise scalar of observation noise
#'
#' @return function with kernel parameters as input and negative log likelihood
#' as output
nll <- function(kernel, X_train, y_train, noise) {
  function(params) {
    n <- dim(X_train)[[1]]
    K <- rlang::exec(kernel, X1 = X_train, X2 = X_train, !!!params)
    L <- chol(K + noise**2 * diag(n))
    a <- backsolve(r = L, x = forwardsolve(l = t(L), x = y_train))
    0.5*t(y_train)%*%a + sum(log(diag(L))) + 0.5*n*log(2*pi)
  }
}

#' Gaussian Process Regression
#'
#' @param kernel kernel function
#' @param X_train matrix (n, d) of training points
#' @param y_train column vector (n, d) of training observations
#' @param noise scalar of observation noise
#' @param ... parameters of the kernel function with initial guesses. Due to the
#' optimiser used, all parameters must be given and the order unfortunately
#' matters
#'
#' @return function that takes a matrix of prediction points as input and
#' returns the posterior predictive distribution for the output
gpr <- function(kernel, X_train, y_train, noise = 1e-8, ...) {
  kernel_nll <- nll(kernel, X_train, y_train, noise)
  param <- list(...)
  opt <- optim(par = rep(1, length(param)), fn = kernel_nll)
  opt_param <- opt$par
  function(X_pred) {
    post <- rlang::exec(
      posterior,
      kernel = kernel,
      X_pred = X_pred,
      X_train = X_train,
      y_train = y_train,
      noise = noise,
      !!!opt_param
    )
    list(
      mu = post$mu,
      sigma = diag(post$sigma),
      Sigma = post$sigma,
      parameters = set_names(opt_param, names(param))
    )
  }
}

#' Expected Improvement Acquisition Function
#' 
#' @mu vector of length m. Mean of a Gaussian process at m points.
#' @sigma vector of length m. The diagonal of the covariance matrix of a
#' Gaussian process evaluated at m points.
#' @param y_best scalar. Best mean prediction so far on observed points
#' @param xi scalar, exploration/exploitation trade off
#' @task one of "max" or "min", indicating the optimisation problem
#'
#' @return EI, vector of length m
expected_improvement <- function(mu, sigma, y_best, xi = 0.01, task = "min") {
  if (task == "min") imp <- y_best - mu - xi
  if (task == "max") imp <- mu - y_best - xi
  if (is.null(imp)) stop('task must be "min" or "max"')
  Z <- imp / sigma
  ei <- imp * pnorm(Z) + sigma * dnorm(Z)
  ei[sigma == 0.0] <- 0.0
  ei
}

uncertainty_plot <- function(mu, sigma, X_train, y_train) {
  tibble::tibble(
    m = mu,
    uncertainty = 1.96*sqrt(sigma),
    upper = m + uncertainty,
    lower = m - uncertainty,
    x = X_pred,
    f = objective_function(X_pred)
  ) %>%
    ggplot(aes(x = x)) +
    geom_line(aes(y = m, colour = "Mean")) +
    geom_ribbon(
      aes(ymin = lower, ymax = upper, fill = "89% interval"),
      alpha = 0.2
    ) +
    geom_point(
      data = tibble::tibble(x = X_train, y = y_train),
      aes(x = x, y = y, shape = "Training point"),
      colour = "#fb8500",
      size = 4
    ) +
    geom_line(mapping = aes(y = f, colour = "True function")) +
    scale_shape_manual(values = c("Training point" = "+")) +
    scale_fill_manual(values = c("89% interval" = "#219ebc")) +
    labs(shape = "") +
    theme_minimal() +
    labs(
      y = "y",
      x = "",
      colour = "",
      fill = ""
    ) +
    theme(panel.grid = element_blank(), axis.text.x = element_blank())
}

acquisition_plot <- function(X_pred,
                             acquisition_function,
                             gp_plot,
                             xt1,
                             label = "EI",
                             title = "") {
  p1 <- tibble::tibble(
    x = X_pred,
    a = acquisition_function
  ) %>%
    ggplot() +
    geom_line(aes(x = x, y = a, colour = label)) +
    geom_vline(xintercept = xt1, linetype = 2) +
    theme_minimal() +
    labs(x = "", y = label, colour = "") +
    theme(panel.grid = element_blank())
  p2 <- gp_plot +
    geom_vline(xintercept = xt1, linetype = 2) +
    labs(title = title)
  aligned_plots <- cowplot::align_plots(p2, p1 , align = "v")
  cowplot::plot_grid(aligned_plots[[1]], aligned_plots[[2]], ncol = 1)
}

gp <- gpr(rbf_kernel, X_train, y_train, noise = 0.1, l = 1, sigma_f = 1)
post_pred <- gp(X_pred)
mu <- post_pred$mu
sigma <- post_pred$sigma
ei <- expected_improvement(mu, sigma, min(y_train))
gp_plot <- uncertainty_plot(mu, sigma, X_train, y_train)
acquisition_plot(X_pred, ei, gp_plot, xt1 = X_pred[which.max(ei)], label = "EI", title = "Gaussian Process Surrogate")
```

Now let's look at some alternative surrogate models.


## Ensembles as Surrogate Models

Random Forests are an ensemble method that combines multiple decision trees to form a powerful and robust model. They can be used as surrogate models in Bayesian optimization to handle large datasets and high-dimensional input spaces efficiently.
Calculating Expected Improvement

To calculate the Expected Improvement (EI) for a Random Forest model, we can use the quantile regression forest variant, which provides estimates of the predictive distribution. The quantile regression forest can estimate multiple quantiles of the predictive distribution, enabling the computation of the Expected Improvement.

Here is some R code to demonstrate this process:

```{r}
train <- dplyr::bind_cols(
  tibble::as_tibble(y_train, .name_repair = ~ "y"),
  tibble::as_tibble(X_train, .name_repair = ~ "x")
)
pred <- tibble::as_tibble(X_pred, .name_repair = ~ "x")
preds <- purrr::map(1:20, function(i) {
  m <- neuralnet::neuralnet(
    y ~ .,
    data = train,
    hidden = c(5, 5),
    linear.output = TRUE,
    threshold = 0.0001,
    stepmax = 1e6
  )
  predict(m, pred)
}) %>%
  do.call(cbind, .) %>%
  list(mean = apply(., 1, mean), sd = apply(., 1, sd))
ei_nn_ens <- expected_improvement(preds$mean, preds$sd, min(y_train))
ens_plot <- uncertainty_plot(preds$mean, preds$sd, X_train, y_train)
acquisition_plot(X_pred, ei_nn_ens, ens_plot, X_pred[which.max(ei_nn_ens)], label = "EI", title = "Neural Network Ensemble Surrogate")
```

## Bayesian Neural Networks as Surrogate Models
Bayesian neural networks (BNNs) are neural networks with weigths and biases that are distributions over parameters rather than deterministic point estimates. BNNs can be an attractive alternative to GPs, as they have the dimensionality reduction capabilities of regular neural networks combined with the probabilistic approach to regression that works so well for GPs. However, as we shall see in a moment, BNNs are not a complete walk in the park.

In order to calculate an acquisition function like Expected Improvement, we need a way to estimate the mean and standard deviation functions. Doing so for a BNN is more challenging than for a GP, as BNNs do not have closed-form expressions for the predictive distribution.

Instead, we have to condition the BNN on our training data and then sample from the resulting predictive posterior. There are a few ways to do this and conditioning BNNs is a subject all on its own and not the focus of this post. For now, we will condition our BNN using Hamiltonian Monte Carlo (HMC) in Stan.

Prior, likelihood, model specification.

```{r stan_model, echo=FALSE}
writeLines(readLines("bnn.stan"))
```


```{r}
# Compile the model
model <- rstan::stan_model("bnn.stan")
# Prepare the data
data <- list(
  N = dim(X_train)[[1]],
  D = dim(X_train)[[2]],
  X = X_train,
  y = as.vector(y_train),
  M = dim(X_pred)[[1]],
  X_pred = X_pred,
  sigma = 0.05
)
# Fit the model using HMC sampling
fit <- rstan::sampling(
  object = model,
  data = data,
  chains = 4,
  cores = 4,
  iter = 4000,
  warmup = 3000,
  seed = seed
)
# Summarize the results
y_pred <- rstan::summary(fit, pars = "y_pred")$summary %>%
  tibble::as_tibble() %>%
  dplyr::mutate(ei = expected_improvement(mean, sd, min(y_train)))


ei_bnn <- expected_improvement(y_pred$mean, y_pred$sd, min(y_train))
bnn_plot <- uncertainty_plot(y_pred$mean, y_pred$sd, X_train, y_train)
acquisition_plot(X_pred, ei_bnn, bnn_plot, X_pred[which.max(ei_bnn)], label = "EI", title = "Bayesian Neural Network Surrogate")
```

In the example above, we first define the black_box_function that we want to optimize. Then, we set the parameters for the Bayesian Neural Network, such as the number of layers and hidden units. The main part of the code is the expected_improvement function, which calculates the Expected Improvement for a given input x, the BNN model, and the current best observed function value best_y. The function computes the improvement for each weight sample drawn from the BNN's posterior distribution and accumulates the total improvement. The Expected Improvement is calculated by dividing the total improvement by the number of samples.

The optimize_acquisition_function function is responsible for finding the next point to evaluate by optimizing the acquisition function. It takes the BNN model and the current best observed function value as inputs and returns the input x_star that maximizes the Expected Improvement.

This example demonstrates how to calculate the Expected Improvement for Bayesian Neural Networks using Monte Carlo methods. Note that this is a simplified example that does not include the complete implementation of a BNN or the optimization of its hyperparameters.

## Student-t Processes

Student-t Processes (TPs) are an alternative to Gaussian Processes (GPs) that can better model heavy-tailed and non-Gaussian noise. They are defined similarly to GPs but use the Student-t distribution instead of the Gaussian distribution.

#### Expected Improvement with Student-t Processes

Calculating the Expected Improvement for a Student-t Process is similar to the process for Gaussian Processes. The main difference is that the predictive distribution is a Student-t distribution rather than a Gaussian distribution.

```{r}
# Define the black-box function (for illustration purposes)
black_box_function <- function(x) {
  return(sin(2 * pi * x))
}

# Fit a Student-t Process model
# ...

# Define the acquisition function (Expected Improvement)
expected_improvement <- function(x, tp_model, best_y) {
  # Obtain the predictive distribution (Student-t)
  predictive_mean, predictive_variance, degrees_of_freedom <- tp_predict(x, tp_model)
  
  # Compute the Expected Improvement
  improvement <- max(0, best_y - predictive_mean)
  return(improvement)
}

# Optimize the acquisition function to find the next point to evaluate
optimize_acquisition_function <- function(tp_model, best_y) {
  # Define the optimization problem
  # ...

  # Find the input that maximizes the expected improvement
  x_star <- optimize(function(x) -expected_improvement(x, tp_model, best_y), lower = 0, upper = 1)$minimum
  
  return(x_star)
}

```

